{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing past buoy data and reanalyses for use in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all past buoy data into a single dataframe\n",
    "\n",
    "This section will collect all of the cleaned buoy data and combine them into a single dataframe. A column to represent the day of year (DOY) as an integer is also added. These data will be used (along with weather reanalyses) as training data for the machine learning model. Also removes buoys deployed outside of the arctic (<64 degrees N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate multiple raw buoy CSV files into a single DataFrame and add a new column with the Day of Year (DOY) as an integer\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing the CSV files\n",
    "folder_path = '../data/cleaned/buoydata/past'\n",
    "\n",
    "# Use glob to get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the list of CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename the lat and lon columns to Latitude and Longitude\n",
    "combined_df.rename(columns={'Lat': 'Latitude', 'Lon': 'Longitude'}, inplace=True)\n",
    "\n",
    "# Pad Month, Day, Hour, Min, and Sec columns with leading zeros\n",
    "combined_df['Month'] = combined_df['Month'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Day'] = combined_df['Day'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Hour'] = combined_df['Hour'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Min'] = combined_df['Min'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Sec'] = combined_df['Sec'].apply(lambda x: f'{x:02}')\n",
    "\n",
    "# Create a new column called datetime by combining Year, Month, Day, Hour, Min, and Sec columns\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df['Year'].astype(str) + '-' +\n",
    "                                         combined_df['Month'].astype(str) + '-' +\n",
    "                                         combined_df['Day'].astype(str) + ' ' +\n",
    "                                         combined_df['Hour'].astype(str) + ':' +\n",
    "                                         combined_df['Min'].astype(str) + ':' +\n",
    "                                         combined_df['Sec'].astype(str))\n",
    "\n",
    "# Add a new column with the Day of Year (DOY) as an integer\n",
    "combined_df['DOY'] = combined_df['datetime'].dt.dayofyear\n",
    "\n",
    "# Iterate through the combined_df by BuoyID\n",
    "for buoy_id, group in combined_df.groupby('BuoyID'):\n",
    "    # Sort the records for each BuoyID by datetime from oldest to newest\n",
    "    group = group.sort_values(by='datetime')\n",
    "    \n",
    "    # Check if the first row of the sorted data has a latitude value less than 64\n",
    "    if group.iloc[0]['Latitude'] < 64:\n",
    "        # Remove the entire BuoyID from the dataset\n",
    "        combined_df = combined_df[combined_df['BuoyID'] != buoy_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate ERA5 to buoy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from scipy.spatial import cKDTree\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Load the NetCDF files\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "# Extract the valid_time, latitudes, longitudes, and u-component wind values from the NetCDF file\n",
    "valid_time = uwnd_ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "latitudes = uwnd_ds.variables['latitude'][:]\n",
    "longitudes = uwnd_ds.variables['longitude'][:]\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Add a column to the dataframe called \"timestamp\"\n",
    "combined_df['timestamp'] = combined_df['datetime'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "\n",
    "# Create a KDTree for fast spatial lookup\n",
    "lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "# Add new columns to combined_df for the u-component and v-component wind values\n",
    "combined_df['era5_uwnd'] = np.nan\n",
    "combined_df['era5_vwnd'] = np.nan\n",
    "\n",
    "# Check the shape of the uwnd_array\n",
    "print(f\"uwnd_array shape: {uwnd_array.shape}\")\n",
    "print(f\"vwnd_array shape: {vwnd_array.shape}\")\n",
    "\n",
    "# Iterate through each row in the dataframe\n",
    "for index, row in combined_df.iterrows():\n",
    "    # Find the value of the netCDF variable valid_time closest to the timestamp value\n",
    "    timestamp = row['timestamp']\n",
    "    time_diffs = np.abs(valid_time - timestamp)\n",
    "    closest_time_index = np.argmin(time_diffs)\n",
    "    \n",
    "    # Check if the calculated index is within the bounds of the uwnd_array\n",
    "    if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "        print(f\"Skipping row {index} with timestamp {timestamp} as it is out of bounds\")\n",
    "        continue\n",
    "    \n",
    "    # Select the corresponding netCDF slices\n",
    "    uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "    vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "    \n",
    "    # Find the grid cell of the netCDF slice closest to the Latitude and Longitude position\n",
    "    lat_lon = (row['Latitude'], row['Longitude'])\n",
    "    _, closest_point_index = tree.query(lat_lon)\n",
    "    closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "    \n",
    "    # Find the index of the closest latitude/longitude pair in the arrays\n",
    "    lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "    lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "    \n",
    "    # Assign the corresponding u and v values to the new columns in the dataframe\n",
    "    combined_df.at[index, 'era5_uwnd'] = uwnd_slice[lat_index, lon_index]\n",
    "    combined_df.at[index, 'era5_vwnd'] = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "# Drop the timestamp column from the dataframe\n",
    "combined_df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "# Print the dataframe head\n",
    "print(combined_df.head())\n",
    "\n",
    "# Print a message saying the script has completed\n",
    "print(\"The ERA5 wind assignment script has completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate IBCAO v5 bathymetry to buoy data\n",
    "\n",
    "NOTE: This data is not currently implemented for use in model training, etc. Time/hardware restraints necessitated skipping this variable in later steps but it is here as a placeholder for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "\n",
    "# Load the georeferenced raster file\n",
    "raster_path = '../data/raw/geospatial/ibcao_v5_2024_100m_depth.tiff'\n",
    "\n",
    "with rasterio.open(raster_path) as raster:\n",
    "    # Get raster metadata\n",
    "    raster_data = raster.read(1)  # Load raster band data\n",
    "    transform = raster.transform  # Affine transformation matrix\n",
    "    nodata = raster.nodata  # NoData value for the raster\n",
    "\n",
    "    # Create a transformer to convert coordinates from WGS 1984 (EPSG:4326) to the raster's CRS (EPSG:3996)\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", raster.crs, always_xy=True)\n",
    "\n",
    "    # Function to get row and column indices for latitude and longitude\n",
    "    def get_row_col(x, y, transform):\n",
    "        col, row = ~transform * (x, y)\n",
    "        return int(row), int(col)\n",
    "\n",
    "    # Create a function to get raster values using numpy indexing\n",
    "    def get_raster_values(latitudes, longitudes):\n",
    "        # Transform WGS 1984 coordinates to the raster CRS\n",
    "        transformed_coords = transformer.transform(longitudes, latitudes)\n",
    "        x_coords, y_coords = transformed_coords\n",
    "\n",
    "        # Get row and column indices\n",
    "        rows, cols = zip(*[get_row_col(x, y, transform) for x, y in zip(x_coords, y_coords)])\n",
    "        rows = np.array(rows)\n",
    "        cols = np.array(cols)\n",
    "\n",
    "        # Ensure indices are within bounds\n",
    "        valid_mask = (\n",
    "            (rows >= 0) & (rows < raster_data.shape[0]) &\n",
    "            (cols >= 0) & (cols < raster_data.shape[1])\n",
    "        )\n",
    "        values = np.full(latitudes.shape, np.nan)  # Initialize output array with NaN\n",
    "        values[valid_mask] = raster_data[rows[valid_mask], cols[valid_mask]]\n",
    "\n",
    "        # Replace nodata values with NaN\n",
    "        if nodata is not None:\n",
    "            values[values == nodata] = np.nan\n",
    "        return values\n",
    "\n",
    "# Assuming combined_df is already defined and contains 'Latitude' and 'Longitude' columns\n",
    "# Example: combined_df = pd.read_csv('path_to_your_combined_df.csv')\n",
    "\n",
    "    # Extract raster values for all lat/lon pairs\n",
    "    combined_df['IBCAOv5_bathymetry'] = get_raster_values(\n",
    "        combined_df['Latitude'].values,\n",
    "        combined_df['Longitude'].values\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more data to the spreadsheet (wind vector and displacement/heading columns) and extract only the columns needed for training to a new spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import great_circle  # For calculating displacement\n",
    "import math  # For trigonometric calculations\n",
    "\n",
    "print(\"Extracting necessary columns...\")\n",
    "\n",
    "# Extract necessary columns (create a new DataFrame to avoid modifying the original)\n",
    "columns_to_extract = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd', 'IBCAOv5_bathymetry']\n",
    "combined_df = combined_df[columns_to_extract].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "print(\"Columns extracted successfully.\")\n",
    "print(combined_df.head())\n",
    "\n",
    "print(\"Calculating wind magnitude and wind angle...\")\n",
    "\n",
    "# Calculate wind magnitude and wind angle\n",
    "combined_df.loc[:, 'wind_magnitude'] = np.sqrt(combined_df['era5_uwnd']**2 + combined_df['era5_vwnd']**2)\n",
    "combined_df.loc[:, 'wind_angle'] = np.degrees(np.arctan2(combined_df['era5_vwnd'], combined_df['era5_uwnd']))\n",
    "\n",
    "print(\"Wind magnitude and wind angle calculated successfully.\")\n",
    "print(combined_df.head())\n",
    "\n",
    "print(\"Displaying the first few rows of the preprocessed data:\")\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(combined_df.head())\n",
    "\n",
    "print(\"Calculating displacement and heading...\")\n",
    "\n",
    "# Initialize displacement and heading columns\n",
    "combined_df.loc[:, 'displacement'] = 0.0\n",
    "combined_df.loc[:, 'heading'] = 0.0\n",
    "\n",
    "# Function to calculate displacement and heading for each group\n",
    "def calculate_displacement_and_heading(group):\n",
    "    group = group.sort_values(by='datetime').reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        # Ensure latitude and longitude values are passed as numeric arguments\n",
    "        prev_point = (group.loc[i-1, 'Latitude'], group.loc[i-1, 'Longitude'])\n",
    "        curr_point = (group.loc[i, 'Latitude'], group.loc[i, 'Longitude'])\n",
    "        \n",
    "        # Calculate displacement\n",
    "        group.loc[i, 'displacement'] = great_circle(prev_point, curr_point).meters\n",
    "        \n",
    "        # Calculate heading\n",
    "        lat1, lon1 = map(math.radians, prev_point)\n",
    "        lat2, lon2 = map(math.radians, curr_point)\n",
    "        \n",
    "        dlon = lon2 - lon1\n",
    "        x = math.sin(dlon) * math.cos(lat2)\n",
    "        y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1) * math.cos(lat2) * math.cos(dlon))\n",
    "        initial_heading = math.atan2(x, y)\n",
    "        initial_heading = math.degrees(initial_heading)\n",
    "        compass_heading = (initial_heading + 360) % 360\n",
    "        \n",
    "        group.loc[i, 'heading'] = compass_heading\n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "combined_df = combined_df.groupby('BuoyID').apply(calculate_displacement_and_heading).reset_index(drop=True)\n",
    "\n",
    "print(\"Displacement and heading calculated successfully.\")\n",
    "print(combined_df.head())\n",
    "\n",
    "# Save the processed combined_df back to the spreadsheet\n",
    "output_csv_path = '../combined_buoy_data.csv'\n",
    "combined_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Processed buoy data saved to {output_csv_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned buoy data geospatial bounds confirmation\n",
    "\n",
    "This cell will analyze and display the minimum and maximum values of the latitude and longitude fields of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the latitude and longitude ranges\n",
    "\n",
    "min_latitude = combined_df['Latitude'].min()\n",
    "max_latitude = combined_df['Latitude'].max()\n",
    "min_longitude = combined_df['Longitude'].min()\n",
    "max_longitude = combined_df['Longitude'].max()\n",
    "\n",
    "print(f\"Latitude: min = {min_latitude}, max = {max_latitude}\")\n",
    "print(f\"Longitude: min = {min_longitude}, max = {max_longitude}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
