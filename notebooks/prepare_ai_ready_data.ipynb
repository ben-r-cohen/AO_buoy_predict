{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing past buoy data and reanalyses for use in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all past buoy data into a single dataframe\n",
    "\n",
    "This section will collect all of the cleaned buoy data and combine them into a single dataframe. A column to represent the day of year (DOY) as an integer is also added. These data will be used (along with weather reanalyses) as training data for the machine learning model. Also removes buoys deployed outside of the arctic (<64 degrees N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the path to the folder containing the CSV files\n",
    "folder_path = '../data/cleaned/buoydata/past'\n",
    "\n",
    "# Use glob to get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through the list of CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Rename the lat and lon columns to Latitude and Longitude\n",
    "combined_df.rename(columns={'Lat': 'Latitude', 'Lon': 'Longitude'}, inplace=True)\n",
    "\n",
    "# Pad Month, Day, Hour, Min, and Sec columns with leading zeros\n",
    "combined_df['Month'] = combined_df['Month'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Day'] = combined_df['Day'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Hour'] = combined_df['Hour'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Min'] = combined_df['Min'].apply(lambda x: f'{x:02}')\n",
    "combined_df['Sec'] = combined_df['Sec'].apply(lambda x: f'{x:02}')\n",
    "\n",
    "# Create a new column called datetime by combining Year, Month, Day, Hour, Min, and Sec columns\n",
    "combined_df['datetime'] = pd.to_datetime(combined_df['Year'].astype(str) + '-' +\n",
    "                                         combined_df['Month'].astype(str) + '-' +\n",
    "                                         combined_df['Day'].astype(str) + ' ' +\n",
    "                                         combined_df['Hour'].astype(str) + ':' +\n",
    "                                         combined_df['Min'].astype(str) + ':' +\n",
    "                                         combined_df['Sec'].astype(str))\n",
    "\n",
    "# Add a new column with the Day of Year (DOY) as an integer\n",
    "combined_df['DOY'] = combined_df['datetime'].dt.dayofyear\n",
    "\n",
    "# Iterate through the combined_df by BuoyID\n",
    "filtered_dfs = []\n",
    "for buoy_id, group in combined_df.groupby('BuoyID'):\n",
    "    # Sort the records for each BuoyID by datetime from oldest to newest\n",
    "    group = group.sort_values(by='datetime')\n",
    "\n",
    "    # Remove duplicate datetime values, keeping only the first occurrence\n",
    "    group = group.drop_duplicates(subset=['datetime'], keep='first')\n",
    "\n",
    "    # Append the filtered group to the list\n",
    "    filtered_dfs.append(group)\n",
    "\n",
    "# Concatenate the filtered groups back into a single DataFrame\n",
    "combined_df = pd.concat(filtered_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate ERA5 to buoy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing variable: ERA5_sea_ice_cover\n",
      "Finished processing ERA5_sea_ice_cover\n",
      "Processing variable: ERA5_10m_u_component_of_wind\n",
      "Finished processing ERA5_10m_u_component_of_wind\n",
      "Processing variable: ERA5_10m_v_component_of_wind\n",
      "Finished processing ERA5_10m_v_component_of_wind\n",
      "Processing variable: ERA5_100m_u_component_of_wind\n",
      "Finished processing ERA5_100m_u_component_of_wind\n",
      "Processing variable: ERA5_100m_v_component_of_wind\n",
      "Finished processing ERA5_100m_v_component_of_wind\n",
      "All variables have been processed and interpolated.\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import cKDTree\n",
    "from datetime import timezone\n",
    "\n",
    "# Define paths to the NetCDF files and their corresponding variables\n",
    "netcdf_files = {\n",
    "    'ERA5_sea_ice_cover': '../data/raw/reanalyses/ERA5/era5_sea_ice_cover_2023.nc',\n",
    "    'ERA5_10m_u_component_of_wind': '../data/raw/reanalyses/ERA5/era5_10m_u_component_of_wind_2023.nc',\n",
    "    'ERA5_10m_v_component_of_wind': '../data/raw/reanalyses/ERA5/era5_10m_v_component_of_wind_2023.nc',\n",
    "    'ERA5_100m_u_component_of_wind': '../data/raw/reanalyses/ERA5/era5_100m_u_component_of_wind_2023.nc',\n",
    "    'ERA5_100m_v_component_of_wind': '../data/raw/reanalyses/ERA5/era5_100m_v_component_of_wind_2023.nc'\n",
    "}\n",
    "\n",
    "# Load the combined DataFrame\n",
    "# combined_df should already exist with 'datetime', 'Latitude', and 'Longitude' columns\n",
    "# Example:\n",
    "# combined_df = pd.read_csv('path_to_combined_df.csv')\n",
    "\n",
    "# Add a timestamp column to combined_df\n",
    "combined_df['timestamp'] = combined_df['datetime'].apply(lambda x: int(x.replace(tzinfo=timezone.utc).timestamp()))\n",
    "\n",
    "# Iterate over each variable and interpolate values\n",
    "for variable, file_path in netcdf_files.items():\n",
    "    print(f\"Processing variable: {variable}\")\n",
    "    \n",
    "    # Open the NetCDF file\n",
    "    ds = nc.Dataset(file_path)\n",
    "    \n",
    "    # Extract the necessary variables\n",
    "    valid_time = ds.variables['valid_time'][:]\n",
    "    latitudes = ds.variables['latitude'][:]\n",
    "    longitudes = ds.variables['longitude'][:]\n",
    "    data_array = ds.variables[list(ds.variables.keys())[-1]][:]  # Assuming last variable is the data\n",
    "\n",
    "    # Check dimensions and adjust for 3D arrays\n",
    "    if len(data_array.shape) == 4:  # Time, Level, Lat, Lon\n",
    "        data_array = data_array[:, 0, :, :]  # Take the first level\n",
    "\n",
    "    # Create a KDTree for spatial lookup\n",
    "    lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "    tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "    # Add a new column for the variable in combined_df\n",
    "    combined_df[variable] = np.nan\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in combined_df.iterrows():\n",
    "        # Find the closest time index\n",
    "        timestamp = row['timestamp']\n",
    "        time_diffs = np.abs(valid_time - timestamp)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Skip if out of bounds\n",
    "        if closest_time_index < 0 or closest_time_index >= data_array.shape[0]:\n",
    "            print(f\"Skipping row {index} due to time out of bounds\")\n",
    "            continue\n",
    "\n",
    "        # Extract the corresponding slice\n",
    "        data_slice = data_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the closest grid point\n",
    "        lat_lon = (row['Latitude'], row['Longitude'])\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find indices of the closest latitude and longitude\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Get the value and check if it's valid\n",
    "        value = data_slice[lat_index, lon_index]\n",
    "        if np.ma.is_masked(value) or np.isnan(value):\n",
    "            # Find the nearest valid value by expanding the search\n",
    "            found_valid_value = False\n",
    "            for neighbor_index in range(2, len(lat_lon_pairs) + 1):\n",
    "                _, expanded_indices = tree.query(lat_lon, k=neighbor_index)\n",
    "                for expanded_index in expanded_indices:\n",
    "                    nearest_lat, nearest_lon = lat_lon_pairs[expanded_index]\n",
    "                    nearest_lat_index = np.where(latitudes == nearest_lat)[0][0]\n",
    "                    nearest_lon_index = np.where(longitudes == nearest_lon)[0][0]\n",
    "                    value = data_slice[nearest_lat_index, nearest_lon_index]\n",
    "                    if not np.ma.is_masked(value) and not np.isnan(value):  # Found a valid value\n",
    "                        found_valid_value = True\n",
    "                        break\n",
    "                if found_valid_value:\n",
    "                    break\n",
    "\n",
    "        # Assign the interpolated value to the DataFrame and round to 3 decimal places\n",
    "        combined_df.at[index, variable] = round(value, 3) if not np.ma.is_masked(value) else np.nan\n",
    "\n",
    "    print(f\"Finished processing {variable}\")\n",
    "\n",
    "# Ensure all interpolated columns are rounded to 3 decimal places\n",
    "combined_df[list(netcdf_files.keys())] = combined_df[list(netcdf_files.keys())].round(3)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "# combined_df.to_csv('path_to_updated_combined_df.csv', index=False)\n",
    "\n",
    "# Print completion message\n",
    "print(\"All variables have been processed and interpolated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaNs and their counts:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in the DataFrame X\n",
    "nan_counts = combined_df.isna().sum()\n",
    "\n",
    "# Print the columns with NaNs and their counts\n",
    "print(\"Columns with NaNs and their counts:\")\n",
    "print(nan_counts[nan_counts > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate IBCAO v5 bathymetry to buoy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pyproj import Transformer\n",
    "\n",
    "# Load the georeferenced raster file\n",
    "raster_path = '../data/raw/geospatial/ibcao_v5_2024_100m_depth.tiff'\n",
    "\n",
    "with rasterio.open(raster_path) as raster:\n",
    "    # Get raster metadata\n",
    "    raster_data = raster.read(1)  # Load raster band data\n",
    "    transform = raster.transform  # Affine transformation matrix\n",
    "    nodata = raster.nodata  # NoData value for the raster\n",
    "\n",
    "    # Create a transformer to convert coordinates from WGS 1984 (EPSG:4326) to the raster's CRS (EPSG:3996)\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", raster.crs, always_xy=True)\n",
    "\n",
    "    # Function to get row and column indices for latitude and longitude\n",
    "    def get_row_col(x, y, transform):\n",
    "        col, row = ~transform * (x, y)\n",
    "        return int(row), int(col)\n",
    "\n",
    "    # Create a function to get raster values using numpy indexing\n",
    "    def get_raster_values(latitudes, longitudes):\n",
    "        # Transform WGS 1984 coordinates to the raster CRS\n",
    "        transformed_coords = transformer.transform(longitudes, latitudes)\n",
    "        x_coords, y_coords = transformed_coords\n",
    "\n",
    "        # Get row and column indices\n",
    "        rows, cols = zip(*[get_row_col(x, y, transform) for x, y in zip(x_coords, y_coords)])\n",
    "        rows = np.array(rows)\n",
    "        cols = np.array(cols)\n",
    "\n",
    "        # Ensure indices are within bounds\n",
    "        valid_mask = (\n",
    "            (rows >= 0) & (rows < raster_data.shape[0]) &\n",
    "            (cols >= 0) & (cols < raster_data.shape[1])\n",
    "        )\n",
    "        values = np.full(latitudes.shape, np.nan)  # Initialize output array with NaN\n",
    "        values[valid_mask] = raster_data[rows[valid_mask], cols[valid_mask]]\n",
    "\n",
    "        # Replace nodata values with NaN\n",
    "        if nodata is not None:\n",
    "            values[values == nodata] = np.nan\n",
    "        \n",
    "        # Round extracted raster values to 3 decimal places\n",
    "        return np.round(values, 3)\n",
    "\n",
    "# Assuming combined_df is already defined and contains 'Latitude' and 'Longitude' columns\n",
    "# Example: combined_df = pd.read_csv('path_to_your_combined_df.csv')\n",
    "\n",
    "# Extract raster values for all lat/lon pairs and round them to 3 decimal places\n",
    "combined_df['IBCAOv5_bathymetry'] = get_raster_values(\n",
    "    combined_df['Latitude'].values,\n",
    "    combined_df['Longitude'].values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add more data to the spreadsheet (wind vector and displacement/heading columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating wind magnitude and wind angle...\n",
      "Wind magnitude and wind angle calculated successfully.\n",
      "            BuoyID  Year Month Day Hour Min Sec  Latitude  Longitude  \\\n",
      "0  300025010734900  2023    08  07   00  07  32  77.33740 -138.15785   \n",
      "1  300025010734900  2023    08  07   00  51  05  77.33538 -138.13705   \n",
      "2  300025010734900  2023    08  07   01  01  40  77.33479 -138.13317   \n",
      "3  300025010734900  2023    08  07   02  01  21  77.33148 -138.11950   \n",
      "4  300025010734900  2023    08  07   03  01  11  77.32867 -138.12018   \n",
      "\n",
      "   GPSdelay  ...  ERA5_sea_ice_cover  ERA5_10m_u_component_of_wind  \\\n",
      "0         0  ...                0.49                         2.133   \n",
      "1         0  ...                0.49                         1.552   \n",
      "2         0  ...                0.49                         1.552   \n",
      "3         0  ...                0.48                         1.249   \n",
      "4         0  ...                0.48                         0.723   \n",
      "\n",
      "   ERA5_10m_v_component_of_wind  ERA5_100m_u_component_of_wind  \\\n",
      "0                        -1.723                          2.605   \n",
      "1                        -2.264                          1.642   \n",
      "2                        -2.264                          1.642   \n",
      "3                        -2.768                          1.250   \n",
      "4                        -3.224                          0.595   \n",
      "\n",
      "   ERA5_100m_v_component_of_wind  IBCAOv5_bathymetry ERA5_wind_magnitude_10m  \\\n",
      "0                         -2.824           -3700.891                   2.742   \n",
      "1                         -3.231           -3691.111                   2.745   \n",
      "2                         -3.231           -3697.584                   2.745   \n",
      "3                         -3.651           -3698.184                   3.037   \n",
      "4                         -4.392           -3699.036                   3.304   \n",
      "\n",
      "   ERA5_wind_angle_10m  ERA5_wind_magnitude_100m  ERA5_wind_angle_100m  \n",
      "0              -38.931                     3.842               -47.310  \n",
      "1              -55.569                     3.624               -63.060  \n",
      "2              -55.569                     3.624               -63.060  \n",
      "3              -65.714                     3.859               -71.100  \n",
      "4              -77.360                     4.432               -82.285  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "Calculating displacement, heading, time differences, and velocity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benem\\AppData\\Local\\Temp\\ipykernel_19288\\1366828196.py:76: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  combined_df = combined_df.groupby('BuoyID', group_keys=False).apply(calculate_movement_metrics).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displacement, heading, time differences, and velocity calculated successfully.\n",
      "   BuoyID  Year Month Day Hour Min Sec  Latitude  Longitude  GPSdelay  ...  \\\n",
      "0  900115  2023    01  01   00  00  46  81.53036 -149.67551         0  ...   \n",
      "1  900115  2023    01  01   00  30  47  81.53165 -149.68448         0  ...   \n",
      "2  900115  2023    01  01   01  01  17  81.53296 -149.69345         0  ...   \n",
      "3  900115  2023    01  01   01  31  03  81.53421 -149.70296         0  ...   \n",
      "4  900115  2023    01  01   02  00  47  81.53523 -149.71284         0  ...   \n",
      "\n",
      "   IBCAOv5_bathymetry  ERA5_wind_magnitude_10m  ERA5_wind_angle_10m  \\\n",
      "0           -3545.648                    7.177              157.054   \n",
      "1           -3530.164                    7.251              158.675   \n",
      "2           -3511.190                    7.251              158.675   \n",
      "3           -3493.298                    7.391              159.047   \n",
      "4           -3475.697                    7.391              159.047   \n",
      "\n",
      "   ERA5_wind_magnitude_100m  ERA5_wind_angle_100m  displacement  heading  \\\n",
      "0                    10.385               151.275       205.313  314.323   \n",
      "1                    10.493               152.741       206.857  314.768   \n",
      "2                    10.493               152.741       208.707  311.762   \n",
      "3                    10.716               153.378       197.533  305.047   \n",
      "4                    10.716               153.378       200.537  305.221   \n",
      "\n",
      "   time_to_next_position  time_to_last_position  velocity  \n",
      "0                 1801.0                    NaN       NaN  \n",
      "1                 1830.0                 1801.0     0.114  \n",
      "2                 1786.0                 1830.0     0.113  \n",
      "3                 1784.0                 1786.0     0.117  \n",
      "4                 1814.0                 1784.0     0.111  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from geopy.distance import great_circle  # For calculating displacement\n",
    "import math  # For trigonometric calculations\n",
    "\n",
    "def add_new_columns(combined_df):\n",
    "    print(\"Calculating wind magnitude and wind angle...\")\n",
    "\n",
    "    # Calculate wind magnitude and wind angle\n",
    "    combined_df.loc[:, 'ERA5_wind_magnitude_10m'] = np.round(\n",
    "        np.sqrt(combined_df['ERA5_10m_u_component_of_wind']**2 + combined_df['ERA5_10m_v_component_of_wind']**2), 3\n",
    "    )\n",
    "    combined_df.loc[:, 'ERA5_wind_angle_10m'] = np.round(\n",
    "        np.degrees(np.arctan2(combined_df['ERA5_10m_v_component_of_wind'], combined_df['ERA5_10m_u_component_of_wind'])), 3\n",
    "    )\n",
    "    combined_df.loc[:, 'ERA5_wind_magnitude_100m'] = np.round(\n",
    "        np.sqrt(combined_df['ERA5_100m_u_component_of_wind']**2 + combined_df['ERA5_100m_v_component_of_wind']**2), 3\n",
    "    )\n",
    "    combined_df.loc[:, 'ERA5_wind_angle_100m'] = np.round(\n",
    "        np.degrees(np.arctan2(combined_df['ERA5_100m_v_component_of_wind'], combined_df['ERA5_100m_u_component_of_wind'])), 3\n",
    "    )\n",
    "\n",
    "    print(\"Wind magnitude and wind angle calculated successfully.\")\n",
    "    print(combined_df.head())\n",
    "\n",
    "    print(\"Calculating displacement, heading, time differences, and velocity...\")\n",
    "\n",
    "    # Function to calculate displacement, heading, time differences, and velocity for each group\n",
    "    def calculate_movement_metrics(group):\n",
    "        group = group.sort_values(by='datetime').reset_index(drop=True)\n",
    "        \n",
    "        # Initialize new columns\n",
    "        group.loc[:, 'displacement'] = np.nan\n",
    "        group.loc[:, 'heading'] = np.nan\n",
    "        group.loc[:, 'time_to_next_position'] = np.nan\n",
    "        group.loc[:, 'time_to_last_position'] = np.nan\n",
    "        group.loc[:, 'velocity'] = np.nan\n",
    "\n",
    "        if len(group) > 1:\n",
    "            for i in range(len(group)):\n",
    "                if i < len(group) - 1:  # Compute for all but the last row\n",
    "                    prev_point = (group.loc[i, 'Latitude'], group.loc[i, 'Longitude'])\n",
    "                    next_point = (group.loc[i + 1, 'Latitude'], group.loc[i + 1, 'Longitude'])\n",
    "\n",
    "                    # Calculate displacement\n",
    "                    displacement = great_circle(prev_point, next_point).meters\n",
    "                    group.loc[i, 'displacement'] = round(displacement, 3)\n",
    "\n",
    "                    # Calculate heading\n",
    "                    lat1, lon1 = map(math.radians, prev_point)\n",
    "                    lat2, lon2 = map(math.radians, next_point)\n",
    "\n",
    "                    dlon = lon2 - lon1\n",
    "                    x = math.sin(dlon) * math.cos(lat2)\n",
    "                    y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1) * math.cos(lat2) * math.cos(dlon))\n",
    "                    initial_heading = math.atan2(x, y)\n",
    "                    initial_heading = math.degrees(initial_heading)\n",
    "                    compass_heading = (initial_heading + 360) % 360\n",
    "\n",
    "                    group.loc[i, 'heading'] = round(compass_heading, 3)\n",
    "\n",
    "                    # Calculate time to next position\n",
    "                    time_diff = (group.loc[i + 1, 'datetime'] - group.loc[i, 'datetime']).total_seconds()\n",
    "                    group.loc[i, 'time_to_next_position'] = round(time_diff, 3)\n",
    "\n",
    "                if i > 0:  # Compute for all but the first row\n",
    "                    prev_time_diff = (group.loc[i, 'datetime'] - group.loc[i - 1, 'datetime']).total_seconds()\n",
    "                    group.loc[i, 'time_to_last_position'] = round(prev_time_diff, 3)\n",
    "\n",
    "                    # Calculate velocity as displacement / time\n",
    "                    if prev_time_diff > 0 and not np.isnan(group.loc[i-1, 'displacement']):\n",
    "                        group.loc[i, 'velocity'] = round(group.loc[i-1, 'displacement'] / prev_time_diff, 3)\n",
    "\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each group\n",
    "    combined_df = combined_df.groupby('BuoyID', group_keys=False).apply(calculate_movement_metrics).reset_index(drop=True)\n",
    "\n",
    "    print(\"Displacement, heading, time differences, and velocity calculated successfully.\")\n",
    "    print(combined_df.head())\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Add new columns for wind magnitude, angle, displacement, heading, time differences, and velocity\n",
    "combined_df = add_new_columns(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated buoy data saved to ../data/processed/interpolated_buoy_data.csv.\n"
     ]
    }
   ],
   "source": [
    "# Save the combined_df to a CSV file\n",
    "output_csv_path = '../data/processed/interpolated_buoy_data.csv'\n",
    "combined_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Interpolated buoy data saved to {output_csv_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
