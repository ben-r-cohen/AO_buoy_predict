{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic Machine Learning\n",
    "This notebook implements a five-fold cross-validation model selection pipeline for classic machine learning algorithms. Once the best model has been selected, it is passed to a hyperparameter selection algorithm using Optuna. The tuned best model is then trained on the entire dataset with five buoys withheld for validation and evaluation of prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import gc\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data Handling\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Geospatial Calculations\n",
    "from geopy import Point\n",
    "from geopy.distance import great_circle\n",
    "from haversine import haversine\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    VotingRegressor\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    ElasticNet,\n",
    "    Lasso,\n",
    "    LinearRegression,\n",
    "    Ridge\n",
    ")\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Model Evaluation and Optimization\n",
    "import optuna\n",
    "from optuna import create_study\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GroupKFold,\n",
    "    KFold,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process ERA5 spatial references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023_pressure1.nc'\n",
    "\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to preload ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Preload all ERA5 data into memory as arrays\n",
    "def load_era5_data(era5_files):\n",
    "    \"\"\"\n",
    "    Preloads ERA5 data into memory and creates KDTree for spatial lookup.\n",
    "\n",
    "    Args:\n",
    "        era5_files (dict): Dictionary where keys are variable names and values are file paths to ERA5 NetCDF files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with preloaded data arrays, latitude/longitude, and KDTree for each variable.\n",
    "    \"\"\"\n",
    "    era5_data = {}\n",
    "    for var_name, file_path in era5_files.items():\n",
    "        with nc.Dataset(file_path) as ds:\n",
    "            # Load data, time, latitude, and longitude\n",
    "            data_array = ds.variables[list(ds.variables.keys())[-1]][:]\n",
    "            valid_time = ds.variables['time'][:]\n",
    "            latitudes = ds.variables['latitude'][:]\n",
    "            longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "            # Adjust for 4D arrays (time, level, lat, lon)\n",
    "            if len(data_array.shape) == 4:  # Time, Level, Lat, Lon\n",
    "                data_array = data_array[:, 0, :, :]  # Use the first level\n",
    "\n",
    "            # Create KDTree for spatial lookup\n",
    "            lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "            tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "            # Store data in dictionary\n",
    "            era5_data[var_name] = {\n",
    "                'data': data_array,\n",
    "                'time': valid_time,\n",
    "                'latitudes': latitudes,\n",
    "                'longitudes': longitudes,\n",
    "                'tree': tree\n",
    "            }\n",
    "    return era5_data\n",
    "\n",
    "era5_files = {\n",
    "    '10m_u_component_of_wind': '../data/raw/reanalyses/ERA5/era5_10m_u_component_of_wind_2023.nc',\n",
    "    '10m_v_component_of_wind': '../data/raw/reanalyses/ERA5/era5_10m_v_component_of_wind_2023.nc',\n",
    "    'mean_wave_direction': '../data/raw/reanalyses/ERA5/era5_mean_wave_direction_2023.nc',\n",
    "    'mean_wave_period': '../data/raw/reanalyses/ERA5/era5_mean_wave_period_2023.nc',\n",
    "    'significant_height_of_combined_wind_waves_and_swell': '../data/raw/reanalyses/ERA5/era5_significant_height_of_combined_wind_waves_and_swell_2023.nc',\n",
    "    '100m_u_component_of_wind': '../data/raw/reanalyses/ERA5/era5_100m_u_component_of_wind_2023.nc',\n",
    "    '100m_v_component_of_wind': '../data/raw/reanalyses/ERA5/era5_100m_v_component_of_wind_2023.nc',\n",
    "    'model_bathymetry': '../data/raw/reanalyses/ERA5/era5_model_bathymetry_2023.nc',\n",
    "    'sea_ice_cover': '../data/raw/reanalyses/ERA5/era5_sea_ice_cover_2023.nc'\n",
    "}\n",
    "\n",
    "# Preload data\n",
    "era5_data = load_era5_data(era5_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract ERA5 data from a given latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract data from preloaded ERA5 arrays\n",
    "def extract_era5_data(lat, lon, dt, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs, era5_data):\n",
    "    \"\"\"\n",
    "    Extracts ERA5 variable values for a given lat, lon, and datetime.\n",
    "\n",
    "    Args:\n",
    "        lat (float): Latitude of the point.\n",
    "        lon (float): Longitude of the point.\n",
    "        dt (datetime): Datetime object for the point.\n",
    "        tree (cKDTree): KDTree for spatial lookup.\n",
    "        valid_time_dt (list[datetime]): List of valid datetime objects corresponding to ERA5 time.\n",
    "        latitudes (np.ndarray): Array of latitude values.\n",
    "        longitudes (np.ndarray): Array of longitude values.\n",
    "        lat_lon_pairs (np.ndarray): Array of (lat, lon) pairs.\n",
    "        era5_data (dict): Dictionary with preloaded ERA5 data arrays.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with variable names as keys and extracted values as values.\n",
    "    \"\"\"\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Find the closest time index\n",
    "    time_diffs = [abs((dt - vt).total_seconds()) for vt in valid_time_dt]\n",
    "    closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "    if closest_time_index < 0 or closest_time_index >= len(valid_time_dt):\n",
    "        return {var_name: np.nan for var_name in era5_data.keys()}\n",
    "\n",
    "    # Find the closest spatial index\n",
    "    _, closest_point_index = tree.query((lat, lon))\n",
    "    closest_lat = latitudes[closest_point_index // len(longitudes)]\n",
    "    closest_lon = longitudes[closest_point_index % len(longitudes)]\n",
    "\n",
    "    lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "    lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "    # Extract data for each variable\n",
    "    for var_name, data in era5_data.items():\n",
    "        data_array = data['data']\n",
    "        extracted_data[var_name] = data_array[closest_time_index, lat_index, lon_index]\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate new position from current position, displacement, and heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the math module\n",
    "import math\n",
    "\n",
    "# Redefine the calculate_new_position function with wrapping logic\n",
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    # Convert inputs to radians\n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    # Compute new latitude\n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    # Compute new longitude\n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    # Convert back to degrees\n",
    "    new_lat = math.degrees(lat2)\n",
    "    new_lon = math.degrees(lon2)\n",
    "    \n",
    "    # Wrap longitude to [-180, 180]\n",
    "    if new_lon > 180:\n",
    "        new_lon -= 360\n",
    "    elif new_lon < -180:\n",
    "        new_lon += 360\n",
    "    \n",
    "    return new_lat, new_lon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prediction(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs, era5_data):\n",
    "    \"\"\"\n",
    "    Predicts buoy motion iteratively using a machine learning model and ERA5 data.\n",
    "\n",
    "    Args:\n",
    "        val_data (pd.DataFrame): Validation dataset with columns ['BuoyID', 'Latitude', 'Longitude', 'datetime'].\n",
    "        model: Trained machine learning model for prediction.\n",
    "        tree (cKDTree): KDTree for spatial lookup of ERA5 data.\n",
    "        valid_times (list): List of valid datetime objects for ERA5 time.\n",
    "        latitudes (np.ndarray): Array of ERA5 latitudes.\n",
    "        longitudes (np.ndarray): Array of ERA5 longitudes.\n",
    "        lat_lon_pairs (np.ndarray): Array of (latitude, longitude) pairs for spatial lookup.\n",
    "        era5_data (dict): Dictionary with preloaded ERA5 data arrays.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of predictions for all buoys.\n",
    "    \"\"\"\n",
    "    # Add a time_to_next_position column to val_data\n",
    "    val_data = val_data.sort_values(by=['BuoyID', 'datetime']).reset_index(drop=True)\n",
    "    val_data['time_to_next_position'] = val_data.groupby('BuoyID')['datetime'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "    # Initialize an empty list to store predictions for all buoys\n",
    "    all_predictions = []\n",
    "\n",
    "    # Iterate over each unique BuoyID\n",
    "    unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "    for buoy_id in unique_buoy_ids:\n",
    "        buoy_data = val_data[val_data['BuoyID'] == buoy_id]\n",
    "\n",
    "        # Initialize an empty list to store predictions for the current buoy\n",
    "        predictions = []\n",
    "\n",
    "        # Extract initial conditions for the current buoy\n",
    "        current_lat, current_lon = buoy_data.iloc[0][['Latitude', 'Longitude']]\n",
    "\n",
    "        # Initialize ERA5 variables for the first row\n",
    "        current_era5_values = {}\n",
    "        for var_name in era5_data.keys():\n",
    "            current_era5_values[var_name] = extract_era5_data(\n",
    "                current_lat,\n",
    "                current_lon,\n",
    "                buoy_data.iloc[0]['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs,\n",
    "                era5_data\n",
    "            )[var_name]\n",
    "\n",
    "        # Add the initial condition as the first prediction\n",
    "        predictions.append([current_lat, current_lon, buoy_data.iloc[0]['datetime']])\n",
    "\n",
    "        for i in range(1, len(buoy_data)):\n",
    "            next_row = buoy_data.iloc[i]\n",
    "            time_to_next_position = next_row['time_to_next_position']\n",
    "\n",
    "            # Prepare input data with all ERA5 variables\n",
    "            input_data = {\"Latitude\": [current_lat], \"Longitude\": [current_lon], \"time_to_next_position\": [time_to_next_position]}\n",
    "            for var_name, value in current_era5_values.items():\n",
    "                input_data[var_name] = [value]\n",
    "            input_data = pd.DataFrame(input_data)\n",
    "\n",
    "            # Make prediction for displacement and heading\n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "\n",
    "            # Calculate new position based on displacement and heading\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "\n",
    "            # Extract all ERA5 data for the predicted position and time\n",
    "            predicted_era5_values = extract_era5_data(\n",
    "                predicted_lat,\n",
    "                predicted_lon,\n",
    "                next_row['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs,\n",
    "                era5_data\n",
    "            )\n",
    "\n",
    "            # Append the prediction for the current buoy\n",
    "            predictions.append([predicted_lat, predicted_lon, next_row['datetime']])\n",
    "\n",
    "            # Update current state for the next iteration\n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_era5_values = predicted_era5_values\n",
    "\n",
    "        # Append predictions of the current buoy to all_predictions\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    # Convert all predictions to a NumPy array before returning\n",
    "    all_predictions_array = np.array(all_predictions, dtype=object)\n",
    "    return all_predictions_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the spreadsheet\n",
    "buoy_data = pd.read_csv('../data/ai_ready/buoydata/combined_buoy_data_IABP2023.csv')\n",
    "\n",
    "# Convert 'datetime' column to datetime type\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "\n",
    "# Explicitly list columns to use for X\n",
    "columns_for_X = [\n",
    "    'Latitude',\n",
    "    'Longitude',\n",
    "    'era5_10m_uwnd',\n",
    "    'era5_10m_vwnd',\n",
    "    'era5_100m_uwnd',\n",
    "    'era5_100m_vwnd',\n",
    "    'BuoyID',\n",
    "    'datetime'\n",
    "]\n",
    "\n",
    "# Define features and targets\n",
    "X = buoy_data[columns_for_X]\n",
    "y = buoy_data[['displacement', 'heading']]\n",
    "groups = buoy_data['BuoyID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to evaluate\n",
    "model_configs = [\n",
    "    ('ElasticNet', MultiOutputRegressor(ElasticNet(alpha=1.0, l1_ratio=0.5))),\n",
    "    ('GradientBoosting', MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100, max_depth=5))),\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=100, max_depth=10)),\n",
    "    ('XGBoost', MultiOutputRegressor(XGBRegressor(n_estimators=100, max_depth=6, objective='reg:squarederror'))),\n",
    "    ('LightGBM', MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=100, max_depth=6, **lgb_params)))\n",
    "]\n",
    "\n",
    "# LightGBM verbosity suppression\n",
    "lgb_params = {'verbose': -1}\n",
    "\n",
    "# GroupKFold for cross-validation\n",
    "cv_folds = 5\n",
    "group_kf = GroupKFold(n_splits=cv_folds)\n",
    "\n",
    "# Ensure the predictions directory exists\n",
    "predictions_dir = '../data/processed/predictions'\n",
    "os.makedirs(predictions_dir, exist_ok=True)\n",
    "\n",
    "# Initialize DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Cross-validation\n",
    "for model_name, model in model_configs:\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "    model_scores = []  # To store RMSE for each fold\n",
    "    fold_times = []  # To store time taken for each fold\n",
    "\n",
    "    for fold_num, (train_index, val_index) in enumerate(group_kf.split(X, y, groups=groups)):\n",
    "        print(f\"\\nFold {fold_num + 1}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Retain 'BuoyID' in X_val for iteration step\n",
    "        X_val_with_buoyid = X_val.copy()\n",
    "        X_train = X_train.drop(columns=['BuoyID', 'datetime'])\n",
    "        X_val = X_val.drop(columns=['BuoyID', 'datetime'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict iteratively\n",
    "        y_pred = iterative_prediction(\n",
    "            val_data=X_val_with_buoyid,\n",
    "            model=model,\n",
    "            tree=tree,\n",
    "            valid_times=valid_time_dt,\n",
    "            latitudes=latitudes,\n",
    "            longitudes=longitudes,\n",
    "            lat_lon_pairs=lat_lon_pairs\n",
    "        )\n",
    "\n",
    "        # Convert predictions to a DataFrame for easier handling\n",
    "        y_pred = pd.DataFrame(y_pred, columns=['Latitude', 'Longitude', 'datetime'])\n",
    "\n",
    "        # Exclude the datetime column for RMSE calculation and ensure numeric dtype\n",
    "        y_pred_numeric = np.array(y_pred[['Latitude', 'Longitude']].to_numpy(), dtype=np.float64)\n",
    "\n",
    "        # Ensure y_val is in the same format\n",
    "        y_val_numeric = y_val.to_numpy()\n",
    "\n",
    "        # Calculate RMSE\n",
    "        try:\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_numeric, y_pred_numeric))\n",
    "            model_scores.append(rmse)\n",
    "            print(f\"Fold {fold_num + 1} RMSE: {rmse:.3f}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error calculating RMSE: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Record time taken for the fold\n",
    "        fold_time = time.time() - start_time\n",
    "        fold_times.append(fold_time)\n",
    "        print(f\"Fold {fold_num + 1} time: {fold_time:.2f} seconds\")\n",
    "\n",
    "        # Save predictions and true values to CSV\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'BuoyID': X_val_with_buoyid['BuoyID'].values,  # Add BuoyID to the output\n",
    "            'True Latitude': X_val_with_buoyid['Latitude'].values,  # Use latitude from X_val_with_buoyid\n",
    "            'True Longitude': X_val_with_buoyid['Longitude'].values,  # Use longitude from X_val_with_buoyid\n",
    "            'Predicted Latitude': np.round(y_pred_numeric[:, 0], 4),  # Predicted latitude rounded to 4 decimal places\n",
    "            'Predicted Longitude': np.round(y_pred_numeric[:, 1], 4)  # Predicted longitude rounded to 4 decimal places\n",
    "        })\n",
    "        predictions_file = os.path.join(predictions_dir, f\"{model_name}_fold{fold_num + 1}_predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "    # Store results for this model\n",
    "    mean_rmse = np.mean(model_scores)\n",
    "    std_rmse = np.std(model_scores)\n",
    "    total_time = sum(fold_times)\n",
    "\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Mean RMSE': mean_rmse,\n",
    "        'RMSE StdDev': std_rmse,\n",
    "        'Total Time (s)': total_time,\n",
    "        'Mean Time per Fold (s)': np.mean(fold_times)\n",
    "    })\n",
    "\n",
    "    print(f\"\\nCompleted cross-validation for {model_name}. \"\n",
    "          f\"Mean RMSE: {mean_rmse:.3f}, Std. Dev: {std_rmse:.3f}, Total Time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "\n",
    "# Identify the best model based on mean RMSE\n",
    "best_model_row = results_df.loc[results_df['Mean RMSE'].idxmin()]\n",
    "print(f\"\\n=== Best model selected: {best_model_row['Model']} ===\")\n",
    "print(f\"Mean RMSE: {best_model_row['Mean RMSE']:.3f}, Total Time: {best_model_row['Total Time (s)']:.2f} seconds\")\n",
    "\n",
    "# Store the best model\n",
    "best_model = model_configs[results_df['Mean RMSE'].idxmin()][1]\n",
    "\n",
    "print(f\"Best model: {best_model_row['Model']}\")\n",
    "print(f\"Mean RMSE: {best_model_row['Mean RMSE']:.3f}\")\n",
    "print(f\"Total Time: {best_model_row['Total Time (s)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning on the best model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Define the objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    print(f\"Starting trial {trial.number}...\")  # Track the start of each trial\n",
    "\n",
    "    if best_model_row['Model'] == 'ElasticNet':\n",
    "        alpha = trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "        l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "        model = MultiOutputRegressor(ElasticNet(alpha=alpha, l1_ratio=l1_ratio))\n",
    "    elif best_model_row['Model'] == 'GradientBoosting':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth))\n",
    "    elif best_model_row['Model'] == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 5, 15)\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    elif best_model_row['Model'] == 'XGBoost':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        model = MultiOutputRegressor(XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, objective='reg:squarederror'))\n",
    "    elif best_model_row['Model'] == 'LightGBM':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        model = MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate))\n",
    "\n",
    "    # Cross-validation logic\n",
    "    model_scores = []\n",
    "    for fold, (train_index, val_index) in enumerate(group_kf.split(X, y, groups=groups)):\n",
    "        print(f\"  Processing fold {fold + 1}...\")  # Track folds\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Convert to numpy arrays and reduce precision\n",
    "        X_train = X_train.drop(columns=['BuoyID', 'datetime']).to_numpy(dtype='float32')\n",
    "        X_val = X_val.drop(columns=['BuoyID', 'datetime']).to_numpy(dtype='float32')\n",
    "        y_train = y_train.to_numpy(dtype='float32')\n",
    "        y_val = y_val.to_numpy(dtype='float32')\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Iterative prediction\n",
    "        y_pred = iterative_prediction(\n",
    "            val_data=X.iloc[val_index],\n",
    "            model=model,\n",
    "            tree=tree,\n",
    "            valid_times=valid_time_dt,\n",
    "            latitudes=latitudes,\n",
    "            longitudes=longitudes,\n",
    "            lat_lon_pairs=lat_lon_pairs\n",
    "        )\n",
    "\n",
    "        # Filter out datetime column from predictions\n",
    "        y_pred_filtered = y_pred[:, :2]  # Keep only Longitude and Latitude\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_filtered))\n",
    "        model_scores.append(rmse)\n",
    "        print(f\"    Fold {fold + 1} RMSE: {rmse:.4f}\")  # Track RMSE per fold\n",
    "\n",
    "        # Free memory after each fold\n",
    "        del X_train, X_val, y_train, y_val, y_pred, y_pred_filtered\n",
    "        gc.collect()\n",
    "\n",
    "    trial_score = np.mean(model_scores)\n",
    "    print(f\"Trial {trial.number} completed with mean RMSE: {trial_score:.4f}\")  # Track trial completion\n",
    "    return trial_score\n",
    "\n",
    "# Create an Optuna study with pruning and enable parallel execution\n",
    "study = optuna.create_study(direction='minimize', pruner=MedianPruner())\n",
    "\n",
    "# Perform optimization with parallel processing\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "study.optimize(objective, n_trials=10, n_jobs=-1)  # n_jobs=-1 utilizes all available CPU cores\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with the best tuned model and saving the results for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from geopy.distance import geodesic\n",
    "import time\n",
    "\n",
    "# Randomly select 5 buoys for validation\n",
    "print(\"Selecting 5 random buoys for validation...\")\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "validation_buoys = np.random.choice(X['BuoyID'].unique(), size=5, replace=False)\n",
    "print(f\"Selected validation buoys: {validation_buoys}\")\n",
    "\n",
    "# Split the data into training and validation based on BuoyID\n",
    "print(\"Splitting data into training and validation sets...\")\n",
    "train_data = X[~X['BuoyID'].isin(validation_buoys)].copy()\n",
    "val_data = X[X['BuoyID'].isin(validation_buoys)].copy()\n",
    "\n",
    "# Ensure y is aligned with the indices of X\n",
    "y_train = y.loc[train_data.index]\n",
    "y_val = y.loc[val_data.index]\n",
    "\n",
    "print(f\"Training data size: {train_data.shape[0]} rows\")\n",
    "print(f\"Validation data size: {val_data.shape[0]} rows\")\n",
    "\n",
    "# Drop 'BuoyID' and 'datetime' for training\n",
    "print(\"Dropping unnecessary columns ('BuoyID', 'datetime') from training and validation sets...\")\n",
    "X_train_clean = train_data.drop(columns=['BuoyID', 'datetime'])\n",
    "X_val_clean = val_data.drop(columns=['BuoyID', 'datetime'])\n",
    "\n",
    "# Instantiate the model using best_params\n",
    "print(\"Instantiating the model with the best parameters...\")\n",
    "if best_model_row['Model'] == 'ElasticNet':\n",
    "    best_model = MultiOutputRegressor(ElasticNet(**best_params))\n",
    "elif best_model_row['Model'] == 'GradientBoosting':\n",
    "    best_model = MultiOutputRegressor(GradientBoostingRegressor(**best_params))\n",
    "elif best_model_row['Model'] == 'RandomForest':\n",
    "    best_model = RandomForestRegressor(**best_params)\n",
    "elif best_model_row['Model'] == 'XGBoost':\n",
    "    best_model = MultiOutputRegressor(XGBRegressor(**best_params, objective='reg:squarederror'))\n",
    "elif best_model_row['Model'] == 'LightGBM':\n",
    "    best_model = MultiOutputRegressor(lgb.LGBMRegressor(**best_params))\n",
    "\n",
    "# Train the tuned model on the training data\n",
    "print(\"Training the model on the training data...\")\n",
    "best_model.fit(X_train_clean, y_train)\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# Use the iterative_prediction function for evaluation on validation buoys\n",
    "print(\"Generating predictions for validation buoys...\")\n",
    "y_pred = iterative_prediction(\n",
    "    val_data=val_data,\n",
    "    model=best_model,\n",
    "    tree=tree,\n",
    "    valid_times=valid_time_dt,\n",
    "    latitudes=latitudes,\n",
    "    longitudes=longitudes,\n",
    "    lat_lon_pairs=lat_lon_pairs\n",
    ")\n",
    "print(\"Predictions generated successfully.\")\n",
    "\n",
    "# Convert predictions to a DataFrame and include BuoyID and Datetime\n",
    "print(\"Preparing predictions DataFrame...\")\n",
    "try:\n",
    "    # Extract only latitude and longitude columns from y_pred\n",
    "    pred_lat_lon = y_pred[:, :2]\n",
    "\n",
    "    # Ensure the extracted data is a NumPy array of float type\n",
    "    pred_lat_lon = np.array(pred_lat_lon, dtype=np.float64)\n",
    "\n",
    "    # Round the latitude and longitude values to 3 decimal places\n",
    "    pred_lat_lon = np.round(pred_lat_lon, 3)\n",
    "\n",
    "    # Create the predictions DataFrame\n",
    "    y_pred_df = pd.DataFrame(\n",
    "        pred_lat_lon, columns=['Predicted Latitude', 'Predicted Longitude']\n",
    "    )\n",
    "\n",
    "    # Add metadata columns (BuoyID and Datetime)\n",
    "    y_pred_df['BuoyID'] = val_data['BuoyID'].values\n",
    "    y_pred_df['Datetime'] = val_data['datetime'].values\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during DataFrame creation: {e}\")\n",
    "    print(f\"y_pred shape: {y_pred.shape}, y_pred content (first rows): {y_pred[:5]}\")\n",
    "    raise\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "\n",
    "# Ensure valid arrays for true and predicted values\n",
    "true_lat_lon = np.array(val_data[['Latitude', 'Longitude']].values, dtype=np.float64)\n",
    "pred_lat_lon = np.array(y_pred_df[['Predicted Latitude', 'Predicted Longitude']].values, dtype=np.float64)\n",
    "\n",
    "# Safeguard against scalar values\n",
    "if true_lat_lon.ndim != 2 or pred_lat_lon.ndim != 2:\n",
    "    raise ValueError(f\"Expected 2D arrays for latitude/longitude, got shapes: \"\n",
    "                     f\"true_lat_lon: {true_lat_lon.shape}, pred_lat_lon: {pred_lat_lon.shape}\")\n",
    "\n",
    "# Calculate metrics\n",
    "lat_lon_rmse = np.sqrt(mean_squared_error(true_lat_lon, pred_lat_lon))\n",
    "lat_lon_mae = mean_absolute_error(true_lat_lon, pred_lat_lon)\n",
    "lat_lon_median_ae = median_absolute_error(true_lat_lon, pred_lat_lon)\n",
    "\n",
    "# Haversine Distance\n",
    "haversine_distances = [\n",
    "    geodesic(true, pred).meters for true, pred in zip(true_lat_lon, pred_lat_lon)\n",
    "]\n",
    "mean_haversine_distance = np.mean(haversine_distances)\n",
    "median_haversine_distance = np.median(haversine_distances)\n",
    "\n",
    "print(f\"Validation Latitude/Longitude RMSE: {lat_lon_rmse:.3f}\")\n",
    "print(f\"Validation Latitude/Longitude MAE: {lat_lon_mae:.3f}\")\n",
    "print(f\"Validation Latitude/Longitude Median AE: {lat_lon_median_ae:.3f}\")\n",
    "print(f\"Mean Haversine Distance: {mean_haversine_distance:.3f} meters\")\n",
    "print(f\"Median Haversine Distance: {median_haversine_distance:.3f} meters\")\n",
    "\n",
    "# Save predictions for analysis\n",
    "print(\"Saving predictions to a CSV file...\")\n",
    "predictions_df = pd.DataFrame({\n",
    "    'BuoyID': val_data['BuoyID'].values,\n",
    "    'Datetime': val_data['datetime'].values,\n",
    "    'True Latitude': np.round(val_data['Latitude'].values, 3),\n",
    "    'True Longitude': np.round(val_data['Longitude'].values, 3),\n",
    "    'Predicted Latitude': y_pred_df['Predicted Latitude'].values,\n",
    "    'Predicted Longitude': y_pred_df['Predicted Longitude'].values,\n",
    "    'Haversine Distance (m)': haversine_distances  # Include Haversine distance for each prediction\n",
    "})\n",
    "\n",
    "# Ensure the output directory exists\n",
    "predictions_file = '../data/processed/predictions/bestmodel_predictions.csv'\n",
    "os.makedirs(os.path.dirname(predictions_file), exist_ok=True)\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"Predictions saved successfully to: {predictions_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlggeo2024_aobuoypredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
