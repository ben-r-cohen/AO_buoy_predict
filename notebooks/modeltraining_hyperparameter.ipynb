{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic Machine Learning\n",
    "This notebook implements a five-fold cross-validation model selection pipeline for classic machine learning algorithms. Once the best model has been selected, it is passed to a hyperparameter selection algorithm using Optuna. The tuned best model is then trained on the entire dataset with five buoys withheld for validation and evaluation of prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benem\\anaconda3\\envs\\mlggeo2024_aobuoypredict\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import gc\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data Handling\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Geospatial Calculations\n",
    "from geopy import Point\n",
    "from geopy.distance import great_circle\n",
    "from haversine import haversine\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    VotingRegressor\n",
    ")\n",
    "from sklearn.linear_model import (\n",
    "    BayesianRidge,\n",
    "    ElasticNet,\n",
    "    Lasso,\n",
    "    LinearRegression,\n",
    "    Ridge\n",
    ")\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Model Evaluation and Optimization\n",
    "import optuna\n",
    "from optuna import create_study\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GroupKFold,\n",
    "    KFold,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    train_test_split\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to pre-process spatial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing KDTree and time differences...\n",
      "KDTree and time differences precomputed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Precompute the KDTree and valid_time differences\n",
    "def precompute_kdtree_and_time_diffs(uwnd_nc_file_path):\n",
    "    try:\n",
    "        print(\"Precomputing KDTree and time differences...\")\n",
    "        # Load the NetCDF file\n",
    "        ds = nc.Dataset(uwnd_nc_file_path)\n",
    "\n",
    "        # Extract the valid_time, latitudes, and longitudes from the NetCDF file\n",
    "        valid_time = ds.variables['valid_time'][:]  # Assuming 'valid_time' is the variable name for time\n",
    "        latitudes = ds.variables['latitude'][:]\n",
    "        longitudes = ds.variables['longitude'][:]\n",
    "\n",
    "        # Convert valid_time from seconds since 1970-01-01 to datetime\n",
    "        base_time = datetime(1970, 1, 1)\n",
    "        valid_time_dt = np.array([base_time + timedelta(seconds=int(ts)) for ts in valid_time], dtype='datetime64[ns]')\n",
    "\n",
    "        # Create a KDTree for fast spatial lookup\n",
    "        lat_lon_pairs = np.array([(lat, lon) for lat in latitudes for lon in longitudes])\n",
    "        tree = cKDTree(lat_lon_pairs)\n",
    "\n",
    "        print(\"KDTree and time differences precomputed successfully.\")\n",
    "        return tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs\n",
    "    except Exception as e:\n",
    "        print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "        raise\n",
    "\n",
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "try:\n",
    "    tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs = precompute_kdtree_and_time_diffs(uwnd_nc_file_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error precomputing KDTree and time differences: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract wind components at a given lat/lon (preloads reanalysis netCDFs also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_uwnd_2023.nc'\n",
    "vwnd_nc_file_path = '../data/raw/reanalyses/ERA5/era5_vwnd_2023.nc'\n",
    "\n",
    "uwnd_ds = nc.Dataset(uwnd_nc_file_path)\n",
    "vwnd_ds = nc.Dataset(vwnd_nc_file_path)\n",
    "\n",
    "uwnd_array = uwnd_ds.variables['u'][:, 0, :, :]  # Assuming 'u' is the variable name for u-component wind and removing the pressure dimension\n",
    "vwnd_array = vwnd_ds.variables['v'][:, 0, :, :]  # Assuming 'v' is the variable name for v-component wind and removing the pressure dimension\n",
    "\n",
    "# Function to extract wind components\n",
    "def extract_wind_components(lat, lon, dt, tree, valid_time_dt, latitudes, longitudes, lat_lon_pairs):\n",
    "    try:\n",
    "        # Convert the given datetime to a numpy datetime64 object\n",
    "        row_datetime = np.datetime64(dt)\n",
    "\n",
    "        # Find the value in the valid_time dimension closest in time to the datetime in the dataframe\n",
    "        time_diffs = np.abs(valid_time_dt - row_datetime)\n",
    "        closest_time_index = np.argmin(time_diffs)\n",
    "\n",
    "        # Check if the calculated index is within the bounds of the uwnd_array\n",
    "        if closest_time_index < 0 or closest_time_index >= uwnd_array.shape[0]:\n",
    "            raise ValueError(\"The given datetime is out of bounds for the NetCDF data\")\n",
    "\n",
    "        # Select the corresponding netCDF slices\n",
    "        uwnd_slice = uwnd_array[closest_time_index, :, :]\n",
    "        vwnd_slice = vwnd_array[closest_time_index, :, :]\n",
    "\n",
    "        # Find the grid cell of the netCDF slice closest to the given Latitude and Longitude position\n",
    "        lat_lon = (lat, lon)\n",
    "        _, closest_point_index = tree.query(lat_lon)\n",
    "        closest_lat, closest_lon = lat_lon_pairs[closest_point_index]\n",
    "\n",
    "        # Find the index of the closest latitude/longitude pair in the arrays\n",
    "        lat_index = np.where(latitudes == closest_lat)[0][0]\n",
    "        lon_index = np.where(longitudes == closest_lon)[0][0]\n",
    "\n",
    "        # Extract the u and v wind components\n",
    "        u_wind = uwnd_slice[lat_index, lon_index]\n",
    "        v_wind = vwnd_slice[lat_index, lon_index]\n",
    "\n",
    "        # Round wind components to 4 decimal places\n",
    "        u_wind = round(u_wind, 4)\n",
    "        v_wind = round(v_wind, 4)\n",
    "\n",
    "        return u_wind, v_wind\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting wind components: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate new position from current position, displacement, and heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the math module\n",
    "import math\n",
    "\n",
    "# Redefine the calculate_new_position function with wrapping logic\n",
    "def calculate_new_position(current_position, displacement, heading):\n",
    "    R = 6371000  # Earth's radius in meters\n",
    "    \n",
    "    # Convert inputs to radians\n",
    "    lat1 = math.radians(current_position[0])\n",
    "    lon1 = math.radians(current_position[1])\n",
    "    heading_rad = math.radians(heading)\n",
    "    \n",
    "    # Compute new latitude\n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(displacement / R) +\n",
    "                     math.cos(lat1) * math.sin(displacement / R) * math.cos(heading_rad))\n",
    "    \n",
    "    # Compute new longitude\n",
    "    lon2 = lon1 + math.atan2(math.sin(heading_rad) * math.sin(displacement / R) * math.cos(lat1),\n",
    "                             math.cos(displacement / R) - math.sin(lat1) * math.sin(lat2))\n",
    "    \n",
    "    # Convert back to degrees\n",
    "    new_lat = math.degrees(lat2)\n",
    "    new_lon = math.degrees(lon2)\n",
    "    \n",
    "    # Wrap longitude to [-180, 180]\n",
    "    if new_lon > 180:\n",
    "        new_lon -= 360\n",
    "    elif new_lon < -180:\n",
    "        new_lon += 360\n",
    "    \n",
    "    return new_lat, new_lon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Evaluation Functions\n",
    "\n",
    "These functions handle model optimization with AutoML and evaluation of predictions. (implementation ongoing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(true_data, predicted_file):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of predictions against true data.\n",
    "    \"\"\"\n",
    "    # Read predicted data\n",
    "    pred_data = pd.read_csv(predicted_file)\n",
    "    pred_data['Datetime'] = pd.to_datetime(pred_data['Datetime'])\n",
    "    \n",
    "    # Merge true and predicted data on datetime\n",
    "    merged_data = pd.merge(\n",
    "        true_data,\n",
    "        pred_data,\n",
    "        left_on=['datetime'],\n",
    "        right_on=['Datetime'],\n",
    "        suffixes=('_true', '_pred')\n",
    "    )\n",
    "    \n",
    "    # Calculate position errors\n",
    "    position_errors = []\n",
    "    for _, row in merged_data.iterrows():\n",
    "        true_pos = (row['Latitude_true'], row['Longitude_true'])\n",
    "        pred_pos = (row['Latitude'], row['Longitude'])\n",
    "        error_km = haversine(true_pos, pred_pos)\n",
    "        position_errors.append(error_km)\n",
    "    \n",
    "    merged_data['position_error_km'] = position_errors\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'mean_position_error_km': np.mean(position_errors),\n",
    "        'median_position_error_km': np.median(position_errors),\n",
    "        'max_position_error_km': np.max(position_errors),\n",
    "        'std_position_error_km': np.std(position_errors),\n",
    "        'rmse_lat': np.sqrt(mean_squared_error(merged_data['Latitude_true'], merged_data['Latitude'])),\n",
    "        'rmse_lon': np.sqrt(mean_squared_error(merged_data['Longitude_true'], merged_data['Longitude'])),\n",
    "        'mae_lat': mean_absolute_error(merged_data['Latitude_true'], merged_data['Latitude']),\n",
    "        'mae_lon': mean_absolute_error(merged_data['Longitude_true'], merged_data['Longitude'])\n",
    "    }\n",
    "    \n",
    "    return metrics, merged_data\n",
    "\n",
    "def plot_trajectory_comparison(merged_data, buoy_id, model_name):\n",
    "    \"\"\"\n",
    "    Plot true vs predicted trajectories\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(merged_data['Longitude_true'], merged_data['Latitude_true'], \n",
    "             'b-', label='True Trajectory')\n",
    "    plt.plot(merged_data['Longitude'], merged_data['Latitude'], \n",
    "             'r--', label='Predicted Trajectory')\n",
    "    plt.title(f'True vs Predicted Trajectory - Buoy {buoy_id}\\nModel: {model_name}')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(f'../data/processed/predictions/trajectory_comparison_{buoy_id}_{model_name}.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_all_predictions(val_data, predictions_dir):\n",
    "    \"\"\"\n",
    "    Evaluate all prediction files in the specified directory\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    prediction_files = glob.glob(f\"{predictions_dir}/predicted_*.csv\")\n",
    "    \n",
    "    for pred_file in prediction_files:\n",
    "        # Extract buoy_id and model_name from filename\n",
    "        filename = pred_file.split('/')[-1]\n",
    "        buoy_id = filename.split('_')[1]\n",
    "        model_name = filename.split('_')[2].replace('.csv', '')\n",
    "        \n",
    "        # Get true data for this buoy\n",
    "        true_data_buoy = val_data[val_data['BuoyID'] == int(buoy_id)]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics, merged_data = evaluate_predictions(true_data_buoy, pred_file)\n",
    "        metrics['buoy_id'] = buoy_id\n",
    "        metrics['model_name'] = model_name\n",
    "        \n",
    "        # Plot trajectory comparison\n",
    "        plot_trajectory_comparison(merged_data, buoy_id, model_name)\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(f'{predictions_dir}/evaluation_results.csv', index=False)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative predictor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_prediction(val_data, model, tree, valid_times, latitudes, longitudes, lat_lon_pairs):\n",
    "\n",
    "    # Initialize an empty list to store predictions for all buoys\n",
    "    all_predictions = []\n",
    "\n",
    "    # Iterate over each unique BuoyID\n",
    "    unique_buoy_ids = val_data['BuoyID'].unique()\n",
    "    for buoy_id in unique_buoy_ids:\n",
    "        buoy_data = val_data[val_data['BuoyID'] == buoy_id]\n",
    "\n",
    "        # Initialize an empty list to store predictions for the current buoy\n",
    "        predictions = []\n",
    "\n",
    "        # Extract initial conditions for the current buoy\n",
    "        current_lat, current_lon = buoy_data.iloc[0][['Latitude', 'Longitude']]\n",
    "        current_uwnd, current_vwnd = buoy_data.iloc[0][['era5_uwnd', 'era5_vwnd']]\n",
    "\n",
    "        # Add the initial condition as the first prediction\n",
    "        predictions.append([current_lat, current_lon, buoy_data.iloc[0]['datetime']])\n",
    "\n",
    "        for i in range(1, len(buoy_data)):\n",
    "            next_row = buoy_data.iloc[i]\n",
    "\n",
    "            # Prepare input data\n",
    "            input_data = pd.DataFrame({\n",
    "                'Latitude': [current_lat],\n",
    "                'Longitude': [current_lon],\n",
    "                'era5_uwnd': [current_uwnd],\n",
    "                'era5_vwnd': [current_vwnd]\n",
    "            })\n",
    "\n",
    "            # Make prediction for displacement and heading\n",
    "            predicted_displacement, predicted_heading = model.predict(input_data)[0]\n",
    "            predicted_lat, predicted_lon = calculate_new_position(\n",
    "                (current_lat, current_lon),\n",
    "                predicted_displacement,\n",
    "                predicted_heading\n",
    "            )\n",
    "\n",
    "            # Extract wind components at the predicted position and time\n",
    "            predicted_wind_u, predicted_wind_v = extract_wind_components(\n",
    "                predicted_lat, \n",
    "                predicted_lon, \n",
    "                next_row['datetime'],\n",
    "                tree,\n",
    "                valid_times,\n",
    "                latitudes,\n",
    "                longitudes,\n",
    "                lat_lon_pairs\n",
    "            )\n",
    "\n",
    "            # Append the prediction for the current buoy\n",
    "            predictions.append([predicted_lat, predicted_lon, next_row['datetime']])\n",
    "\n",
    "            # Update current state for the next iteration\n",
    "            current_lat, current_lon = predicted_lat, predicted_lon\n",
    "            current_uwnd, current_vwnd = predicted_wind_u, predicted_wind_v\n",
    "\n",
    "        # Append predictions of the current buoy to all_predictions\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    # Convert all predictions to a NumPy array before returning\n",
    "    all_predictions_array = np.array(all_predictions, dtype=object)\n",
    "    return all_predictions_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection, training, and validation (includes computation timing calculation)\n",
    "\n",
    "NOTE: This script is extremely computationally intensive. Once the script has begun, interrupting it will crash the Python kernel and force you to re-run the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing model: ElasticNet\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 827.064\n",
      "Fold 1 time: 380.83 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 767.791\n",
      "Fold 2 time: 408.17 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 825.619\n",
      "Fold 3 time: 413.79 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 813.975\n",
      "Fold 4 time: 409.12 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1829.535\n",
      "Fold 5 time: 414.53 seconds\n",
      "\n",
      "Completed cross-validation for ElasticNet. Mean RMSE: 1012.797, Std. Dev: 408.938, Total Time: 2026.44 seconds\n",
      "\n",
      "Testing model: GradientBoosting\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 824.468\n",
      "Fold 1 time: 984.29 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 768.219\n",
      "Fold 2 time: 949.54 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 821.637\n",
      "Fold 3 time: 927.72 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 818.564\n",
      "Fold 4 time: 928.62 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1830.465\n",
      "Fold 5 time: 930.42 seconds\n",
      "\n",
      "Completed cross-validation for GradientBoosting. Mean RMSE: 1012.671, Std. Dev: 409.423, Total Time: 4720.60 seconds\n",
      "\n",
      "Testing model: RandomForest\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 824.421\n",
      "Fold 1 time: 1031.53 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 766.104\n",
      "Fold 2 time: 1012.37 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 825.877\n",
      "Fold 3 time: 1013.98 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 822.726\n",
      "Fold 4 time: 1011.73 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1828.945\n",
      "Fold 5 time: 1010.71 seconds\n",
      "\n",
      "Completed cross-validation for RandomForest. Mean RMSE: 1013.615, Std. Dev: 408.290, Total Time: 5080.32 seconds\n",
      "\n",
      "Testing model: XGBoost\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 824.237\n",
      "Fold 1 time: 1087.02 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 766.923\n",
      "Fold 2 time: 1057.80 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 826.941\n",
      "Fold 3 time: 1061.85 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 812.149\n",
      "Fold 4 time: 1090.89 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1830.533\n",
      "Fold 5 time: 1145.53 seconds\n",
      "\n",
      "Completed cross-validation for XGBoost. Mean RMSE: 1012.157, Std. Dev: 409.756, Total Time: 5443.08 seconds\n",
      "\n",
      "Testing model: LightGBM\n",
      "\n",
      "Fold 1\n",
      "Fold 1 RMSE: 824.533\n",
      "Fold 1 time: 787.57 seconds\n",
      "\n",
      "Fold 2\n",
      "Fold 2 RMSE: 772.126\n",
      "Fold 2 time: 808.36 seconds\n",
      "\n",
      "Fold 3\n",
      "Fold 3 RMSE: 829.211\n",
      "Fold 3 time: 676.46 seconds\n",
      "\n",
      "Fold 4\n",
      "Fold 4 RMSE: 816.268\n",
      "Fold 4 time: 666.25 seconds\n",
      "\n",
      "Fold 5\n",
      "Fold 5 RMSE: 1830.936\n",
      "Fold 5 time: 667.43 seconds\n",
      "\n",
      "Completed cross-validation for LightGBM. Mean RMSE: 1014.615, Std. Dev: 408.663, Total Time: 3606.07 seconds\n",
      "\n",
      "=== Best model selected: XGBoost ===\n",
      "Mean RMSE: 1012.157, Total Time: 5443.08 seconds\n",
      "Best model: XGBoost\n",
      "Mean RMSE: 1012.157\n",
      "Total Time: 5443.08 seconds\n"
     ]
    }
   ],
   "source": [
    "# LightGBM verbosity suppression\n",
    "lgb_params = {'verbose': -1}\n",
    "\n",
    "# Load the data from the spreadsheet\n",
    "buoy_data = pd.read_csv('../combined_buoy_data.csv')\n",
    "\n",
    "# Drop unused columns\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd', 'displacement', 'heading']\n",
    "buoy_data = buoy_data[columns_to_keep].copy()\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "\n",
    "# Define features and targets\n",
    "X = buoy_data[['Latitude', 'Longitude', 'era5_uwnd', 'era5_vwnd', 'BuoyID', 'datetime']]\n",
    "y = buoy_data[['displacement', 'heading']]\n",
    "groups = buoy_data['BuoyID']\n",
    "\n",
    "# Models to evaluate\n",
    "model_configs = [\n",
    "    ('ElasticNet', MultiOutputRegressor(ElasticNet(alpha=1.0, l1_ratio=0.5))),\n",
    "    ('GradientBoosting', MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100, max_depth=5))),\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=100, max_depth=10)),\n",
    "    ('XGBoost', MultiOutputRegressor(XGBRegressor(n_estimators=100, max_depth=6, objective='reg:squarederror'))),\n",
    "    ('LightGBM', MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=100, max_depth=6, **lgb_params)))\n",
    "]\n",
    "\n",
    "# GroupKFold for cross-validation\n",
    "cv_folds = 5\n",
    "group_kf = GroupKFold(n_splits=cv_folds)\n",
    "\n",
    "# Ensure the predictions directory exists\n",
    "predictions_dir = '../data/processed/predictions'\n",
    "os.makedirs(predictions_dir, exist_ok=True)\n",
    "\n",
    "# Initialize DataFrame to store results\n",
    "results = []\n",
    "\n",
    "# Cross-validation\n",
    "for model_name, model in model_configs:\n",
    "    print(f\"\\nTesting model: {model_name}\")\n",
    "    model_scores = []  # To store RMSE for each fold\n",
    "    fold_times = []  # To store time taken for each fold\n",
    "\n",
    "    for fold_num, (train_index, val_index) in enumerate(group_kf.split(X, y, groups=groups)):\n",
    "        print(f\"\\nFold {fold_num + 1}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Retain 'BuoyID' in X_val for iteration step\n",
    "        X_val_with_buoyid = X_val.copy()\n",
    "        X_train = X_train.drop(columns=['BuoyID', 'datetime'])\n",
    "        X_val = X_val.drop(columns=['BuoyID', 'datetime'])\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict iteratively\n",
    "        y_pred = iterative_prediction(\n",
    "            val_data=X_val_with_buoyid,\n",
    "            model=model,\n",
    "            tree=tree,\n",
    "            valid_times=valid_time_dt,\n",
    "            latitudes=latitudes,\n",
    "            longitudes=longitudes,\n",
    "            lat_lon_pairs=lat_lon_pairs\n",
    "        )\n",
    "\n",
    "        # Convert predictions to a DataFrame for easier handling\n",
    "        y_pred = pd.DataFrame(y_pred, columns=['Latitude', 'Longitude', 'datetime'])\n",
    "\n",
    "        # Exclude the datetime column for RMSE calculation and ensure numeric dtype\n",
    "        y_pred_numeric = np.array(y_pred[['Latitude', 'Longitude']].to_numpy(), dtype=np.float64)\n",
    "\n",
    "        # Ensure y_val is in the same format\n",
    "        y_val_numeric = y_val.to_numpy()\n",
    "\n",
    "        # Calculate RMSE\n",
    "        try:\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_numeric, y_pred_numeric))\n",
    "            model_scores.append(rmse)\n",
    "            print(f\"Fold {fold_num + 1} RMSE: {rmse:.3f}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Error calculating RMSE: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Record time taken for the fold\n",
    "        fold_time = time.time() - start_time\n",
    "        fold_times.append(fold_time)\n",
    "        print(f\"Fold {fold_num + 1} time: {fold_time:.2f} seconds\")\n",
    "\n",
    "        # Save predictions and true values to CSV\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'BuoyID': X_val_with_buoyid['BuoyID'].values,  # Add BuoyID to the output\n",
    "            'True Latitude': X_val_with_buoyid['Latitude'].values,  # Use latitude from X_val_with_buoyid\n",
    "            'True Longitude': X_val_with_buoyid['Longitude'].values,  # Use longitude from X_val_with_buoyid\n",
    "            'Predicted Latitude': np.round(y_pred_numeric[:, 0], 4),  # Predicted latitude rounded to 4 decimal places\n",
    "            'Predicted Longitude': np.round(y_pred_numeric[:, 1], 4)  # Predicted longitude rounded to 4 decimal places\n",
    "        })\n",
    "        predictions_file = os.path.join(predictions_dir, f\"{model_name}_fold{fold_num + 1}_predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_file, index=False)\n",
    "\n",
    "    # Store results for this model\n",
    "    mean_rmse = np.mean(model_scores)\n",
    "    std_rmse = np.std(model_scores)\n",
    "    total_time = sum(fold_times)\n",
    "\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Mean RMSE': mean_rmse,\n",
    "        'RMSE StdDev': std_rmse,\n",
    "        'Total Time (s)': total_time,\n",
    "        'Mean Time per Fold (s)': np.mean(fold_times)\n",
    "    })\n",
    "\n",
    "    print(f\"\\nCompleted cross-validation for {model_name}. \"\n",
    "          f\"Mean RMSE: {mean_rmse:.3f}, Std. Dev: {std_rmse:.3f}, Total Time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "\n",
    "# Identify the best model based on mean RMSE\n",
    "best_model_row = results_df.loc[results_df['Mean RMSE'].idxmin()]\n",
    "print(f\"\\n=== Best model selected: {best_model_row['Model']} ===\")\n",
    "print(f\"Mean RMSE: {best_model_row['Mean RMSE']:.3f}, Total Time: {best_model_row['Total Time (s)']:.2f} seconds\")\n",
    "\n",
    "# Store the best model\n",
    "best_model = model_configs[results_df['Mean RMSE'].idxmin()][1]\n",
    "\n",
    "print(f\"Best model: {best_model_row['Model']}\")\n",
    "print(f\"Mean RMSE: {best_model_row['Mean RMSE']:.3f}\")\n",
    "print(f\"Total Time: {best_model_row['Total Time (s)']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoofing the previous result (necessary for resumation of project procedures in the case of kernel crashes, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: XGBoost\n",
      "Mean RMSE: 1012.157\n",
      "Total Time: 5443.08 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Spoof the previous result\n",
    "best_model_row = {\n",
    "    'Model': 'XGBoost',\n",
    "    'Mean RMSE': 1012.157,  \n",
    "    'Total Time (s)': 5443.08 \n",
    "}\n",
    "\n",
    "# Print the spoofed result to ensure it's working as expected\n",
    "print(f\"Best model: {best_model_row['Model']}\")\n",
    "print(f\"Mean RMSE: {best_model_row['Mean RMSE']}\")\n",
    "print(f\"Total Time: {best_model_row['Total Time (s)']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spoofing cont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM verbosity suppression\n",
    "lgb_params = {'verbose': -1}\n",
    "\n",
    "# Load the data from the spreadsheet\n",
    "buoy_data = pd.read_csv('../combined_buoy_data.csv')\n",
    "\n",
    "# Drop unused columns\n",
    "columns_to_keep = ['Latitude', 'Longitude', 'BuoyID', 'datetime', 'era5_uwnd', 'era5_vwnd', 'displacement', 'heading']\n",
    "buoy_data = buoy_data[columns_to_keep].copy()\n",
    "buoy_data['datetime'] = pd.to_datetime(buoy_data['datetime'])\n",
    "\n",
    "# Define features and targets\n",
    "X = buoy_data[['Latitude', 'Longitude', 'era5_uwnd', 'era5_vwnd', 'BuoyID', 'datetime']]\n",
    "y = buoy_data[['displacement', 'heading']]\n",
    "groups = buoy_data['BuoyID']\n",
    "\n",
    "# Models to evaluate\n",
    "model_configs = [\n",
    "    ('ElasticNet', MultiOutputRegressor(ElasticNet(alpha=1.0, l1_ratio=0.5))),\n",
    "    ('GradientBoosting', MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100, max_depth=5))),\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=100, max_depth=10)),\n",
    "    ('XGBoost', MultiOutputRegressor(XGBRegressor(n_estimators=100, max_depth=6, objective='reg:squarederror'))),\n",
    "    ('LightGBM', MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=100, max_depth=6, **lgb_params)))\n",
    "]\n",
    "\n",
    "# GroupKFold for cross-validation\n",
    "cv_folds = 5\n",
    "group_kf = GroupKFold(n_splits=cv_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning on the best model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-28 13:24:58,750] A new study created in memory with name: no-name-df630455-a5f2-49ff-885a-a22e25ff1ce7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning...\n",
      "Starting trial 0...\n",
      "Starting trial 1...\n",
      "Starting trial 2...\n",
      "Starting trial 3...\n",
      "Starting trial 4...\n",
      "Starting trial 5...\n",
      "Starting trial 6...\n",
      "Starting trial 7...\n",
      "Starting trial 8...\n",
      "Starting trial 9...\n",
      "  Processing fold 1...\n",
      "  Processing fold 1...\n",
      "  Processing fold 1...  Processing fold 1...\n",
      "\n",
      "  Processing fold 1...\n",
      "  Processing fold 1...\n",
      "  Processing fold 1...\n",
      "  Processing fold 1...\n",
      "  Processing fold 1...\n",
      "  Processing fold 1...\n",
      "    Fold 1 RMSE: 824.6651\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 826.0157\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 825.4650\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 824.9197\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 825.8214\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 824.7591\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 825.4667\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 825.2453\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 826.6707\n",
      "  Processing fold 2...\n",
      "    Fold 1 RMSE: 824.9205\n",
      "  Processing fold 2...\n",
      "    Fold 2 RMSE: 768.3914\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 765.4216\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 766.2951\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 773.2237\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 772.8171\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 770.7700\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 773.6690\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 769.3809\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 763.4643\n",
      "  Processing fold 3...\n",
      "    Fold 2 RMSE: 763.5479\n",
      "  Processing fold 3...\n",
      "    Fold 3 RMSE: 824.9413\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 827.5040\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 824.8690\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 829.6226\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 828.5457\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 827.6613\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 825.4641\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 828.4710\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 830.3294\n",
      "  Processing fold 4...\n",
      "    Fold 3 RMSE: 825.6376\n",
      "  Processing fold 4...\n",
      "    Fold 4 RMSE: 813.2760\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 813.4201\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 813.8952\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 814.4249\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 816.0758\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 815.0214\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 812.5894\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 814.3776\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 814.8716\n",
      "  Processing fold 5...\n",
      "    Fold 4 RMSE: 813.6988\n",
      "  Processing fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:12:03,779] Trial 4 finished with value: 1011.8157467065827 and parameters: {'n_estimators': 105, 'max_depth': 6, 'learning_rate': 0.08032856349779974}. Best is trial 4 with value: 1011.8157467065827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 5 RMSE: 1830.6307\n",
      "Trial 4 completed with mean RMSE: 1011.8157\n",
      "    Fold 5 RMSE: 1830.6201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:12:37,810] Trial 6 finished with value: 1013.1614311939305 and parameters: {'n_estimators': 81, 'max_depth': 5, 'learning_rate': 0.011056453615366158}. Best is trial 4 with value: 1011.8157467065827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 completed with mean RMSE: 1013.1614\n",
      "    Fold 5 RMSE: 1831.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:12:41,839] Trial 5 finished with value: 1012.3207287954328 and parameters: {'n_estimators': 119, 'max_depth': 5, 'learning_rate': 0.020981171305557045}. Best is trial 4 with value: 1011.8157467065827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 completed with mean RMSE: 1012.3207\n",
      "    Fold 5 RMSE: 1831.7514\n",
      "Trial 3 completed with mean RMSE: 1014.7885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:12:44,079] Trial 3 finished with value: 1014.7884512728033 and parameters: {'n_estimators': 160, 'max_depth': 8, 'learning_rate': 0.06263396969414492}. Best is trial 4 with value: 1011.8157467065827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 5 RMSE: 1831.1293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:37:35,181] Trial 8 finished with value: 1014.8778872818657 and parameters: {'n_estimators': 85, 'max_depth': 10, 'learning_rate': 0.024529933246486172}. Best is trial 4 with value: 1011.8157467065827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 completed with mean RMSE: 1014.8779\n",
      "    Fold 5 RMSE: 1832.7756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:38:28,790] Trial 0 finished with value: 1014.2947019623929 and parameters: {'n_estimators': 112, 'max_depth': 10, 'learning_rate': 0.07514341487560008}. Best is trial 4 with value: 1011.8157467065827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 completed with mean RMSE: 1014.2947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:38:43,134] Trial 9 finished with value: 1011.3034407694342 and parameters: {'n_estimators': 82, 'max_depth': 6, 'learning_rate': 0.2541334093676109}. Best is trial 9 with value: 1011.3034407694342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 5 RMSE: 1830.2403\n",
      "Trial 9 completed with mean RMSE: 1011.3034\n",
      "    Fold 5 RMSE: 1830.3181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:38:54,844] Trial 2 finished with value: 1013.9424337275152 and parameters: {'n_estimators': 54, 'max_depth': 3, 'learning_rate': 0.01283325539257429}. Best is trial 9 with value: 1011.3034407694342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 completed with mean RMSE: 1013.9424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:38:57,873] Trial 7 finished with value: 1015.1000700608416 and parameters: {'n_estimators': 179, 'max_depth': 9, 'learning_rate': 0.03726743632388828}. Best is trial 9 with value: 1011.3034407694342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 5 RMSE: 1831.6577\n",
      "Trial 7 completed with mean RMSE: 1015.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 02:39:13,806] Trial 1 finished with value: 1011.3993309347736 and parameters: {'n_estimators': 147, 'max_depth': 6, 'learning_rate': 0.07611857934040783}. Best is trial 9 with value: 1011.3034407694342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fold 5 RMSE: 1829.1918\n",
      "Trial 1 completed with mean RMSE: 1011.3993\n",
      "Best parameters: {'n_estimators': 82, 'max_depth': 6, 'learning_rate': 0.2541334093676109}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Define the objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    print(f\"Starting trial {trial.number}...\")  # Track the start of each trial\n",
    "\n",
    "    if best_model_row['Model'] == 'ElasticNet':\n",
    "        alpha = trial.suggest_float('alpha', 0.1, 10.0, log=True)\n",
    "        l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
    "        model = MultiOutputRegressor(ElasticNet(alpha=alpha, l1_ratio=l1_ratio))\n",
    "    elif best_model_row['Model'] == 'GradientBoosting':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=n_estimators, max_depth=max_depth))\n",
    "    elif best_model_row['Model'] == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 5, 15)\n",
    "        model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    elif best_model_row['Model'] == 'XGBoost':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        model = MultiOutputRegressor(XGBRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, objective='reg:squarederror'))\n",
    "    elif best_model_row['Model'] == 'LightGBM':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "        max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
    "        model = MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate))\n",
    "\n",
    "    # Cross-validation logic\n",
    "    model_scores = []\n",
    "    for fold, (train_index, val_index) in enumerate(group_kf.split(X, y, groups=groups)):\n",
    "        print(f\"  Processing fold {fold + 1}...\")  # Track folds\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        # Convert to numpy arrays and reduce precision\n",
    "        X_train = X_train.drop(columns=['BuoyID', 'datetime']).to_numpy(dtype='float32')\n",
    "        X_val = X_val.drop(columns=['BuoyID', 'datetime']).to_numpy(dtype='float32')\n",
    "        y_train = y_train.to_numpy(dtype='float32')\n",
    "        y_val = y_val.to_numpy(dtype='float32')\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Iterative prediction\n",
    "        y_pred = iterative_prediction(\n",
    "            val_data=X.iloc[val_index],\n",
    "            model=model,\n",
    "            tree=tree,\n",
    "            valid_times=valid_time_dt,\n",
    "            latitudes=latitudes,\n",
    "            longitudes=longitudes,\n",
    "            lat_lon_pairs=lat_lon_pairs\n",
    "        )\n",
    "\n",
    "        # Filter out datetime column from predictions\n",
    "        y_pred_filtered = y_pred[:, :2]  # Keep only Longitude and Latitude\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred_filtered))\n",
    "        model_scores.append(rmse)\n",
    "        print(f\"    Fold {fold + 1} RMSE: {rmse:.4f}\")  # Track RMSE per fold\n",
    "\n",
    "        # Free memory after each fold\n",
    "        del X_train, X_val, y_train, y_val, y_pred, y_pred_filtered\n",
    "        gc.collect()\n",
    "\n",
    "    trial_score = np.mean(model_scores)\n",
    "    print(f\"Trial {trial.number} completed with mean RMSE: {trial_score:.4f}\")  # Track trial completion\n",
    "    return trial_score\n",
    "\n",
    "# Create an Optuna study with pruning and enable parallel execution\n",
    "study = optuna.create_study(direction='minimize', pruner=MedianPruner())\n",
    "\n",
    "# Perform optimization with parallel processing\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "study.optimize(objective, n_trials=10, n_jobs=-1)  # n_jobs=-1 utilizes all available CPU cores\n",
    "\n",
    "# Retrieve the best parameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions with the best tuned model and saving the results for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 5 random buoys for validation...\n",
      "Selected validation buoys: [300534063058450          900126 300234067977270 300534063050430\n",
      " 300434066254600]\n",
      "Splitting data into training and validation sets...\n",
      "Training data size: 1568182 rows\n",
      "Validation data size: 30424 rows\n",
      "Dropping unnecessary columns ('BuoyID', 'datetime') from training and validation sets...\n",
      "Instantiating the model with the best parameters...\n",
      "Training the model on the training data...\n",
      "Model training completed.\n",
      "Generating predictions for validation buoys...\n",
      "Predictions generated successfully.\n",
      "Preparing predictions DataFrame...\n",
      "Calculating evaluation metrics...\n",
      "Validation Latitude/Longitude RMSE: 48.552\n",
      "Validation Latitude/Longitude MAE: 28.511\n",
      "Validation Latitude/Longitude Median AE: 18.578\n",
      "Mean Haversine Distance: 1446983.693 meters\n",
      "Median Haversine Distance: 1216817.940 meters\n",
      "Saving predictions to a CSV file...\n",
      "Predictions saved successfully to: ../data/processed/predictions/validation_predictions_lat_lon.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from geopy.distance import geodesic\n",
    "import time\n",
    "\n",
    "# Randomly select 5 buoys for validation\n",
    "print(\"Selecting 5 random buoys for validation...\")\n",
    "np.random.seed(42)  # Set seed for reproducibility\n",
    "validation_buoys = np.random.choice(X['BuoyID'].unique(), size=5, replace=False)\n",
    "print(f\"Selected validation buoys: {validation_buoys}\")\n",
    "\n",
    "# Split the data into training and validation based on BuoyID\n",
    "print(\"Splitting data into training and validation sets...\")\n",
    "train_data = X[~X['BuoyID'].isin(validation_buoys)].copy()\n",
    "val_data = X[X['BuoyID'].isin(validation_buoys)].copy()\n",
    "\n",
    "# Ensure y is aligned with the indices of X\n",
    "y_train = y.loc[train_data.index]\n",
    "y_val = y.loc[val_data.index]\n",
    "\n",
    "print(f\"Training data size: {train_data.shape[0]} rows\")\n",
    "print(f\"Validation data size: {val_data.shape[0]} rows\")\n",
    "\n",
    "# Drop 'BuoyID' and 'datetime' for training\n",
    "print(\"Dropping unnecessary columns ('BuoyID', 'datetime') from training and validation sets...\")\n",
    "X_train_clean = train_data.drop(columns=['BuoyID', 'datetime'])\n",
    "X_val_clean = val_data.drop(columns=['BuoyID', 'datetime'])\n",
    "\n",
    "# Instantiate the model using best_params\n",
    "print(\"Instantiating the model with the best parameters...\")\n",
    "if best_model_row['Model'] == 'ElasticNet':\n",
    "    best_model = MultiOutputRegressor(ElasticNet(**best_params))\n",
    "elif best_model_row['Model'] == 'GradientBoosting':\n",
    "    best_model = MultiOutputRegressor(GradientBoostingRegressor(**best_params))\n",
    "elif best_model_row['Model'] == 'RandomForest':\n",
    "    best_model = RandomForestRegressor(**best_params)\n",
    "elif best_model_row['Model'] == 'XGBoost':\n",
    "    best_model = MultiOutputRegressor(XGBRegressor(**best_params, objective='reg:squarederror'))\n",
    "elif best_model_row['Model'] == 'LightGBM':\n",
    "    best_model = MultiOutputRegressor(lgb.LGBMRegressor(**best_params))\n",
    "\n",
    "# Train the tuned model on the training data\n",
    "print(\"Training the model on the training data...\")\n",
    "best_model.fit(X_train_clean, y_train)\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# Use the iterative_prediction function for evaluation on validation buoys\n",
    "print(\"Generating predictions for validation buoys...\")\n",
    "y_pred = iterative_prediction(\n",
    "    val_data=val_data,\n",
    "    model=best_model,\n",
    "    tree=tree,\n",
    "    valid_times=valid_time_dt,\n",
    "    latitudes=latitudes,\n",
    "    longitudes=longitudes,\n",
    "    lat_lon_pairs=lat_lon_pairs\n",
    ")\n",
    "print(\"Predictions generated successfully.\")\n",
    "\n",
    "# Convert predictions to a DataFrame and include BuoyID and Datetime\n",
    "print(\"Preparing predictions DataFrame...\")\n",
    "try:\n",
    "    # Extract only latitude and longitude columns from y_pred\n",
    "    pred_lat_lon = y_pred[:, :2]\n",
    "\n",
    "    # Ensure the extracted data is a NumPy array of float type\n",
    "    pred_lat_lon = np.array(pred_lat_lon, dtype=np.float64)\n",
    "\n",
    "    # Round the latitude and longitude values to 3 decimal places\n",
    "    pred_lat_lon = np.round(pred_lat_lon, 3)\n",
    "\n",
    "    # Create the predictions DataFrame\n",
    "    y_pred_df = pd.DataFrame(\n",
    "        pred_lat_lon, columns=['Predicted Latitude', 'Predicted Longitude']\n",
    "    )\n",
    "\n",
    "    # Add metadata columns (BuoyID and Datetime)\n",
    "    y_pred_df['BuoyID'] = val_data['BuoyID'].values\n",
    "    y_pred_df['Datetime'] = val_data['datetime'].values\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during DataFrame creation: {e}\")\n",
    "    print(f\"y_pred shape: {y_pred.shape}, y_pred content (first rows): {y_pred[:5]}\")\n",
    "    raise\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "\n",
    "# Ensure valid arrays for true and predicted values\n",
    "true_lat_lon = np.array(val_data[['Latitude', 'Longitude']].values, dtype=np.float64)\n",
    "pred_lat_lon = np.array(y_pred_df[['Predicted Latitude', 'Predicted Longitude']].values, dtype=np.float64)\n",
    "\n",
    "# Safeguard against scalar values\n",
    "if true_lat_lon.ndim != 2 or pred_lat_lon.ndim != 2:\n",
    "    raise ValueError(f\"Expected 2D arrays for latitude/longitude, got shapes: \"\n",
    "                     f\"true_lat_lon: {true_lat_lon.shape}, pred_lat_lon: {pred_lat_lon.shape}\")\n",
    "\n",
    "# Calculate metrics\n",
    "lat_lon_rmse = np.sqrt(mean_squared_error(true_lat_lon, pred_lat_lon))\n",
    "lat_lon_mae = mean_absolute_error(true_lat_lon, pred_lat_lon)\n",
    "lat_lon_median_ae = median_absolute_error(true_lat_lon, pred_lat_lon)\n",
    "\n",
    "# Haversine Distance\n",
    "haversine_distances = [\n",
    "    geodesic(true, pred).meters for true, pred in zip(true_lat_lon, pred_lat_lon)\n",
    "]\n",
    "mean_haversine_distance = np.mean(haversine_distances)\n",
    "median_haversine_distance = np.median(haversine_distances)\n",
    "\n",
    "print(f\"Validation Latitude/Longitude RMSE: {lat_lon_rmse:.3f}\")\n",
    "print(f\"Validation Latitude/Longitude MAE: {lat_lon_mae:.3f}\")\n",
    "print(f\"Validation Latitude/Longitude Median AE: {lat_lon_median_ae:.3f}\")\n",
    "print(f\"Mean Haversine Distance: {mean_haversine_distance:.3f} meters\")\n",
    "print(f\"Median Haversine Distance: {median_haversine_distance:.3f} meters\")\n",
    "\n",
    "# Save predictions for analysis\n",
    "print(\"Saving predictions to a CSV file...\")\n",
    "predictions_df = pd.DataFrame({\n",
    "    'BuoyID': val_data['BuoyID'].values,\n",
    "    'Datetime': val_data['datetime'].values,\n",
    "    'True Latitude': np.round(val_data['Latitude'].values, 3),\n",
    "    'True Longitude': np.round(val_data['Longitude'].values, 3),\n",
    "    'Predicted Latitude': y_pred_df['Predicted Latitude'].values,\n",
    "    'Predicted Longitude': y_pred_df['Predicted Longitude'].values,\n",
    "    'Haversine Distance (m)': haversine_distances  # Include Haversine distance for each prediction\n",
    "})\n",
    "\n",
    "# Ensure the output directory exists\n",
    "predictions_file = '../data/processed/predictions/bestmodel_predictions.csv'\n",
    "os.makedirs(os.path.dirname(predictions_file), exist_ok=True)\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"Predictions saved successfully to: {predictions_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlggeo2024_aobuoypredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
