{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw buoy data cleaning\n",
    "This cell will clean the buoy data to remove rows with NAs in the Lat/Lon columns, ensure that the format of lat/lon pairs is standardized, remove buoy locations on land, and remove buoy tracks with less than 50 rows (these are assumed to have been destroyed or instruments failed)\n",
    "\n",
    "The resulting cleaned data will be stored in the data/cleaned/buoydata/past folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300025010024370.csv has been deleted for having less than 50 rows\n",
      "300234066028200.csv has been deleted for starting below 64N\n",
      "300234066336960.csv has been deleted for starting below 64N\n",
      "300234066391330.csv has been deleted for starting below 64N\n",
      "300234066436470.csv has been deleted for starting below 64N\n",
      "300234066533040.csv has been deleted for starting below 64N\n",
      "300234067874480.csv has been deleted for starting below 64N\n",
      "300234067877380.csv has been deleted for starting below 64N\n",
      "300234067878170.csv has been deleted for starting below 64N\n",
      "300234067878310.csv has been deleted for starting below 64N\n",
      "300234067976260.csv has been deleted for starting below 64N\n",
      "300234068241260.csv has been deleted for starting below 64N\n",
      "300234068342720.csv has been deleted for starting below 64N\n",
      "300234068343550.csv has been deleted for starting below 64N\n",
      "300234068345210.csv has been deleted for starting below 64N\n",
      "300234068345410.csv has been deleted for starting below 64N\n",
      "300234068345490.csv has been deleted for starting below 64N\n",
      "300234068348190.csv has been deleted for starting below 64N\n",
      "300234068349700.csv has been deleted for starting below 64N\n",
      "300234068481030.csv has been deleted for starting below 64N\n",
      "300234068482590.csv has been deleted for starting below 64N\n",
      "300234068763470.csv has been deleted for starting below 64N\n",
      "300234068769540.csv has been deleted for starting below 64N\n",
      "300234068818610.csv has been deleted for starting below 64N\n",
      "300534060716640.csv has been deleted for starting below 64N\n",
      "300534061383140.csv has been deleted for starting below 64N\n",
      "300534061383190.csv has been deleted for starting below 64N\n",
      "300534062831790.csv has been deleted for starting below 64N\n",
      "300534062833770.csv has been deleted for starting below 64N\n",
      "300534063016400.csv has been deleted for starting below 64N\n",
      "300534063016420.csv has been deleted for starting below 64N\n",
      "300534063016430.csv has been deleted for starting below 64N\n",
      "300534063016440.csv has been deleted for starting below 64N\n",
      "300534063909060.csv has been deleted for starting below 64N\n",
      "All files have been cleaned and saved to the cleaned directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# Initialize directories and paths\n",
    "input_directory = '../data/raw/buoydata/past'\n",
    "output_directory = '../data/cleaned/buoydata/past'\n",
    "raster_path = '../data/raw/geospatial/arctic_land.tif'\n",
    "\n",
    "# Clear the output directory if it exists, otherwise create it\n",
    "if os.path.exists(output_directory):\n",
    "    shutil.rmtree(output_directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Normalize latitude values\n",
    "def normalize_lat(lat):\n",
    "    while lat < -90 or lat > 90:\n",
    "        if lat < -90:\n",
    "            lat = -180 - lat\n",
    "        elif lat > 90:\n",
    "            lat = 180 - lat\n",
    "    return lat\n",
    "\n",
    "# Normalize longitude values\n",
    "def normalize_lon(lon):\n",
    "    while lon < -180 or lon > 180:\n",
    "        if lon < -180:\n",
    "            lon += 360\n",
    "        elif lon > 180:\n",
    "            lon -= 360\n",
    "    return lon\n",
    "\n",
    "# Open the raster file\n",
    "with rasterio.open(raster_path) as src:\n",
    "    raster_data = src.read(1)\n",
    "    affine_transform = src.transform\n",
    "\n",
    "    def overlaps_raster_value_one(lat, lon):\n",
    "        if pd.isna(lat) or pd.isna(lon):\n",
    "            return False\n",
    "        try:\n",
    "            row, col = src.index(lon, lat)\n",
    "            if 0 <= row < raster_data.shape[0] and 0 <= col < raster_data.shape[1]:\n",
    "                return raster_data[row, col] == 1\n",
    "        except ValueError:\n",
    "            return False\n",
    "        return False\n",
    "\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            input_file_path = os.path.join(input_directory, filename)\n",
    "\n",
    "            df = pd.read_csv(input_file_path)\n",
    "\n",
    "            # Drop rows with NaN in Lat or Lon\n",
    "            df_cleaned = df.dropna(subset=['Lat', 'Lon'])\n",
    "\n",
    "            # Remove duplicates\n",
    "            df_cleaned = df_cleaned.drop_duplicates()\n",
    "\n",
    "            # Normalize Lat and Lon\n",
    "            df_cleaned['Lat'] = df_cleaned['Lat'].apply(normalize_lat)\n",
    "            df_cleaned['Lon'] = df_cleaned['Lon'].apply(normalize_lon)\n",
    "\n",
    "            # Check for overlap with raster value of 1\n",
    "            df_cleaned['overlaps'] = df_cleaned.apply(lambda row: overlaps_raster_value_one(row['Lat'], row['Lon']), axis=1)\n",
    "            removed_rows = df_cleaned[df_cleaned['overlaps']]\n",
    "            df_cleaned = df_cleaned[~df_cleaned['overlaps']]\n",
    "\n",
    "            removed_rows_df = pd.concat([removed_rows_df, removed_rows], ignore_index=True) if 'removed_rows_df' in locals() else removed_rows\n",
    "\n",
    "            # Drop the 'overlaps' column\n",
    "            df_cleaned = df_cleaned.drop(columns=['overlaps'])\n",
    "\n",
    "            # Check if the first location of each buoy is below 64 degrees north\n",
    "            buoy_ids_below_64 = df_cleaned.groupby('BuoyID').first().query('Lat < 64').index\n",
    "            if not buoy_ids_below_64.empty:\n",
    "                df_cleaned = df_cleaned[~df_cleaned['BuoyID'].isin(buoy_ids_below_64)]\n",
    "\n",
    "            # Save the cleaned data or print the appropriate message\n",
    "            if len(df_cleaned) > 50:\n",
    "                output_file_path = os.path.join(output_directory, filename)\n",
    "                df_cleaned.to_csv(output_file_path, index=False)\n",
    "            else:\n",
    "                if not buoy_ids_below_64.empty:\n",
    "                    print(f\"{filename} has been deleted for starting below 64N\")\n",
    "                else:\n",
    "                    print(f\"{filename} has been deleted for having less than 50 rows\")\n",
    "\n",
    "print('All files have been cleaned and saved to the cleaned directory.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
